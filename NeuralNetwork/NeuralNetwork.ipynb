{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wettervorhersagen mit Hilfe eines Neuronalen Netzes\n",
    "\n",
    "Diese Datei ist der \"Neuronale Netz\"-Teil im Vergleich zwischen Linear Regression, Random Forest und Neuronalen Netzwerken im Bezug darauf, welches Modell besser in der Lage dazu ist Wetterdaten (Temperatur und Niederschlag) mit Hilfe von Datensätzen aus drei Wetterstationen, 3 Tage in die Zukunft vorherzusagen. Die zugrundeliegenden Daten liegen in CSV Format vor.\n",
    "\n",
    "## Datenstruktur\n",
    "Jede Zeile in den vorliegenden CSV Dateien repräsentieren (Wetter)daten eines Tags in chronologischer Reihenfolge. Die verfügbaren Spalten umfassen die folgenden Features:\n",
    "- `DATE`\n",
    "- `MESS_DATUM`\n",
    "- `QUALITAETS_NIVEAU`\n",
    "- `LUFTTEMPERATUR`\n",
    "- `DAMPFDRUCK`\n",
    "- `BEDECKUNGSGRAD`\n",
    "- `LUFTDRUCK_STATIONSHOEHE`\n",
    "- `REL_FEUCHTE`\n",
    "- `WINDGESCHWINDIGKEIT`\n",
    "- `LUFTTEMPERATUR_MAXIMUM`\n",
    "- `LUFTTEMPERATUR_MINIMUM`\n",
    "- `LUFTTEMP_AM_ERDB_MINIMUM`\n",
    "- `WINDSPITZE_MAXIMUM`\n",
    "- `NIEDERSCHLAGSHOEHE`\n",
    "- `NIEDERSCHLAGSHOEHE_IND`\n",
    "- `SONNENSCHEINDAUER`\n",
    "- `SCHNEEHOEHE`\n",
    "\n",
    "Es ist dabei wichtig zu beachten, dass fehlende Daten mit dem WErt -999 gekennzeichnet sind. Weiterhin ist zu beachten, dass die Wetterstationen unterschiedliche Daten als Aufzeichnungsbeginn haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der nötigen Bibliotheken\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten importieren\n",
    "Nachdem wir die für unser Programm nötigen Bibliotheken importiert haben müssen wir nun die Wetterdaten einlesen. Zusätzlich werden die Leerzeichen am Anfang und Ende der Spaltennamen entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "arber_data = pd.read_csv(cwd + \"/data/Arber.csv\")\n",
    "schorndorf_data = pd.read_csv(cwd + \"/data/Schorndorf.csv\")\n",
    "straubing_data = pd.read_csv(cwd + \"/data/Straubing.csv\")\n",
    "\n",
    "# Strippen\n",
    "arber_data.columns = arber_data.columns.str.strip()\n",
    "schorndorf_data.columns = schorndorf_data.columns.str.strip()\n",
    "straubing_data.columns = straubing_data.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "Um unsere Daten gezielt weiterverarbeiten zu können splitten wir sie pro Wetterstation in Numeric und Non-Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split numeric and non numeric columns\n",
    "arber_numeric = arber_data.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "arber_non_numeric = arber_data.select_dtypes(exclude=[\"float64\", \"int64\"])\n",
    "\n",
    "schorndorf_numeric = schorndorf_data.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "schorndorf_non_numeric = schorndorf_data.select_dtypes(exclude=[\"float64\", \"int64\"])\n",
    "\n",
    "straubing_numeric = straubing_data.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "straubing_non_numeric = straubing_data.select_dtypes(exclude=[\"float64\", \"int64\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handeling missing values\n",
    "Nachdem wir nun Zugriff auf die numerischen Werte haben können wir uns nun mit den Fehlenden Werten beschäftigen. Da wir einige leere Werte haben füllen wir diese mit dem mean value des columns mit Hilfe eines Imputers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with the mean value only on numeric data\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "arber_numeric_filled = pd.DataFrame(imputer.fit_transform(arber_numeric.replace), columns=arber_numeric.columns)\n",
    "schorndorf_numeric_filled = pd.DataFrame(imputer.fit_transform(schorndorf_numeric), columns=schorndorf_numeric.columns)\n",
    "straubing_numeric_filled = pd.DataFrame(imputer.fit_transform(straubing_numeric), columns=straubing_numeric.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining back together numeric and non-numeric data\n",
    "Nachdem wir die fehlenden Werte mit einem Durchschnittswert gefüllt haben können wir die numerischen und nicht numerischen Daten wieder zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining back together the numeric and non numeric data\n",
    "arber_data_filled = pd.concat([arber_non_numeric.reset_index(drop=True), arber_numeric_filled.reset_index(drop=True)], axis=1)\n",
    "schorndorf_data_filled = pd.concat([schorndorf_non_numeric.reset_index(drop=True), schorndorf_numeric_filled.reset_index(drop=True)], axis=1)\n",
    "straubing_data_filled = pd.concat([straubing_non_numeric.reset_index(drop=True), straubing_numeric_filled.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features\n",
    "Nach der Rückkombination der Daten suchen wir nun die relevanten Features aus unseren Daten aus. Für das Neuronale Netzwerk haben wir uns für die folgenden Features entschieden:\n",
    "- `DATE`\n",
    "- `LUFTTEMPERATUR`\n",
    "- `DAMPFDRUCK`\n",
    "- `REL_FEUCHTE`\n",
    "- `WINDGESCHWINDIGKEIT`\n",
    "- `LUFTTEMPERATUR_MAXIMUM`\n",
    "- `LUFTTEMPERATUR_MINIMUM`\n",
    "- `LUFTTEMP_AM_ERDB_MINIMUM`\n",
    "- `NIEDERSCHLAGSHOEHE`\n",
    "- `NIEDERSCHLAGSHOEHE_IND`\n",
    "- `SCHNEEHOEHE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant features\n",
    "arber_data_selected = arber_data_filled[[\"DATE\", \"LUFTTEMPERATUR\",  \"REL_FEUCHTE\", \"LUFTTEMPERATUR_MAXIMUM\", \"LUFTTEMPERATUR_MINIMUM\", \"LUFTTEMP_AM_ERDB_MINIMUM\", \"NIEDERSCHLAGSHOEHE\", \"DAMPFDRUCK\", \"NIEDERSCHLAGSHOEHE_IND\", \"SCHNEEHOEHE\"]]\n",
    "schorndorf_data_selected = schorndorf_data_filled[[\"DATE\", \"LUFTTEMPERATUR\", \"REL_FEUCHTE\", \"NIEDERSCHLAGSHOEHE\", \"LUFTTEMPERATUR_MAXIMUM\", \"LUFTTEMPERATUR_MINIMUM\", \"LUFTTEMP_AM_ERDB_MINIMUM\", \"DAMPFDRUCK\", \"NIEDERSCHLAGSHOEHE_IND\", \"SCHNEEHOEHE\"]]\n",
    "straubing_data_selected = straubing_data_filled[[\"DATE\", \"LUFTTEMPERATUR\", \"REL_FEUCHTE\", \"NIEDERSCHLAGSHOEHE\", \"LUFTTEMPERATUR_MAXIMUM\", \"LUFTTEMPERATUR_MINIMUM\", \"LUFTTEMP_AM_ERDB_MINIMUM\", \"DAMPFDRUCK\", \"NIEDERSCHLAGSHOEHE_IND\", \"SCHNEEHOEHE\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Beachten des TimeLags\n",
    "Da Daten der Wetterstationen Schorndorf und Arber nur mit einem TimeLag von 3 Tagen genutzt werden dürfen müssen die jeweiligen Daten noch entsprechend verschoben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = \"%d.%m.%Y\"\n",
    "\n",
    "# Incorporating Time Lag\n",
    "# Shift Arber and Schorndorf data by 3 days\n",
    "arber_data_shifted = arber_data_selected.copy()\n",
    "arber_data_shifted['DATE'] = pd.to_datetime(arber_data_shifted['DATE'], format=date_format) + pd.DateOffset(days=3)\n",
    "schorndorf_data_shifted = schorndorf_data_selected.copy()\n",
    "schorndorf_data_shifted['DATE'] = pd.to_datetime(schorndorf_data_shifted['DATE'], format=date_format) + pd.DateOffset(days=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging der Daten\n",
    "Nun können wir alle Daten zu einem Datensatz zusammenführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Data\n",
    "# Convert 'Date' to datetime for merging\n",
    "straubing_data_selected.loc[:, 'DATE'] = pd.to_datetime(straubing_data['DATE'], format=date_format)\n",
    "\n",
    "# Add Prefix to data\n",
    "arber_data_prefixed = arber_data_shifted.add_prefix(\"arber_\")\n",
    "schorndorf_data_prefixed = schorndorf_data_shifted.add_prefix(\"schorndorf_\")\n",
    "straubing_data_prefixed = straubing_data_selected.add_prefix(\"straubing_\")\n",
    "\n",
    "merged_data = straubing_data_prefixed.merge(arber_data_prefixed, left_on='straubing_DATE', right_on=\"arber_DATE\", how='left')\n",
    "merged_data = merged_data.merge(schorndorf_data_prefixed, how=\"left\", left_on=\"straubing_DATE\", right_on=\"schorndorf_DATE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Löschen und Füllen von Daten\n",
    "Um alle NaN Daten aus unserem Datensatz zu haben gehen wir nun über die Daten und löschen NaN Daten zudem ersetzen wir -999 (fehlender Messwert) Werte mit einem Durchschnittswert der jeweiligen Zeile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data[~np.any(np.isnan(merged_data), axis=1)]\n",
    "\n",
    "column_means = merged_data.replace(-999, np.nan).mean()\n",
    "# Replace NaN values with the corresponding column mean values\n",
    "# merged_data.fillna(column_means, inplace=True) \n",
    "merged_data.replace(-999, column_means, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the data for the Neural Network\n",
    "Da wir unserem Neuronalen Netz keine \"Date\" Werte füttern können sondern lediglich numerische müssen wir uns eine neue Variable mit nur diesen Werten bilden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_data = merged_data.copy()\n",
    "used_data.drop(columns=[\"schorndorf_DATE\", \"straubing_DATE\", \"arber_DATE\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalisation\n",
    "Theoretisch haben wir die Möglichkeit nun unsere Daten zu normalisieren. Allerdings haben wir uns dafür entschieden dies nicht zu tun, da es den Realitätsbezug der Daten signifikant stört/zerstört."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization (Using Min-Max Scaling)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_data = scaler.fit_transform(used_data.select_dtypes(include=['float64', 'int64']))\n",
    "scaled_data = used_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Target Day Columns\n",
    "Um unserem Modell zu ermöglichen Vorhersagen für drei Tage im Vorraus zu machen müssen wir die relevanten Straubing Daten (entweder Temperatur oder Niederschlag) auf drei neue (verschobene) Spalten aufteilen. Am Ende müssen wir die letzten 3 Spalten dropen, da diese auf Grund der Verschiebung nun NaN Werte enthalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scaled_data to DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=column_names)\n",
    "\n",
    "scaled_data[\"Target_Day1\"] = scaled_data[\"straubing_LUFTTEMPERATUR\"].shift(-1)\n",
    "scaled_data[\"Target_Day2\"] = scaled_data[\"straubing_LUFTTEMPERATUR\"].shift(-2)\n",
    "scaled_data[\"Target_Day3\"] = scaled_data[\"straubing_LUFTTEMPERATUR\"].shift(-3)\n",
    "\n",
    "# scaled_data[\"Target_Day1\"] = scaled_data[\"straubing_NIEDERSCHLAGSHOEHE\"].shift(-1)\n",
    "# scaled_data[\"Target_Day2\"] = scaled_data[\"straubing_NIEDERSCHLAGSHOEHE\"].shift(-2)\n",
    "# scaled_data[\"Target_Day3\"] = scaled_data[\"straubing_NIEDERSCHLAGSHOEHE\"].shift(-3)\n",
    "\n",
    "scaled_data = scaled_data[:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting X and y\n",
    "Nun sind unsere Daten soweit vorbereitet, dass wir unser X und y selecten können. Wichtig zu beachten ist, dass X weder die Target_Day noch die originale LUFTTEMPERATUR / NIEDERSCHLAGSHOEHE enthält, da diese Werte ja die sind, die das Neuronale Netzwerk benennen soll. Genau aus diesem Grund besteht y aus diesen Werten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_data.drop(columns=[\"straubing_LUFTTEMPERATUR\", \"Target_Day1\", \"Target_Day2\", \"Target_Day3\"], axis=1)\n",
    "# X = scaled_data.drop(columns=[\"straubing_NIEDERSCHLAGSHOEHE\", \"Target_Day1\", \"Target_Day2\", \"Target_Day3\"], axis=1)\n",
    "y = scaled_data[[\"Target_Day1\", \"Target_Day2\", \"Target_Day3\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split\n",
    "Wir haben uns hier für einen 80/20 Train-Test-Split entschieden wir teilen die X und y Werte also jeweils im Verhältnis 80/20 in X_train, X_test und y_train, y_test auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(scaled_data)*0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates Train and Test\n",
    "Um unsere Graphen im Anschluss labeln zu können brauchen wir die korrespondierenden Date Daten zu unseren Werten. Diese bekommen wir aus der vorher angelegten merged_data Variable. Um konsistent zu den anderen Daten zu sein müssen wir auch hier die letzten 3 Spalten droppen und können anschließen unsere dates_train und dates_test auf dem straubing_DATE column mit Hilfe der train_size bilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data[:-3]\n",
    "dates = merged_data[[\"straubing_DATE\"]]\n",
    "dates_train, dates_test = dates[:train_size], dates[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "Jetzt sind wir so weit, dass wir das Neuronale Netz erstellen können. Wir haben eine auskommentierte Loss Function, die für die Regendaten genutzt werden kann. Die Funktion bestraft negative Werte (die es beim Niederschlag nicht geben kann) stärker und sollte so dem Neuronalen Netz helfen besser zu fitten. Ansonten sind die Werte etwas durch Trial and Error entstanden. Wir haben zwei Hidden Layers, einmal mit 64 und einmal mit 32 Neuronen. Und eine Output Layer mit 3 Neuronen (für jeden Tag ein Neuron.) Auch die Activation Functions sind durch Trial and Error herausgefunden worden. Anfangs war die Hoffnung da, dass das einsetzen einer sigmoid Aktivierungsfunktion oder einer tanh Aktivierungsfunktion das Ergebnis des NN verbessern würde, dies hat sich allerdings nicht bewahrheitet. Für das NN zur Voraussage der Temperatur benutzen wir den MSE als loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "model.add(Dense(32, activation='elu'))  # Adjust number of neurons\n",
    "model.add(Dense(3))\n",
    "\n",
    "# def modified_mse(y_true, y_pred):\n",
    "#     penalty_weight = 10.0  # Adjust the penalty weight\n",
    "#     squared_error = tf.square(y_true - y_pred)\n",
    "#     penalty = tf.where(y_pred < 0, penalty_weight * squared_error, squared_error)\n",
    "#     return tf.reduce_mean(penalty)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# model.compile(optimizer='adam', loss=modified_mse)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=250, batch_size=32)  # Adjust epochs and batch_size as needed\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate MSE and R2\n",
    "Nach den Predictions können wir nun die Qualität des NN berechnen dafür berechnen wir einmal den MSE und den R2 Score mit Hilfe der provideden Functions von Sklearn. Um das allerdings tun zu können müssen wir die y_test Daten noch entsprechend in die Daten für die jeweiligen Tagespredictions spalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the actual values for comparison\n",
    "N = len(predictions)\n",
    "actual_day1 = y_test.iloc[:N, 0]\n",
    "actual_day2 = y_test.iloc[:N, 1]\n",
    "actual_day3 = y_test.iloc[:N, 2]\n",
    "\n",
    "\n",
    "# Calculate Mean squared error and R2 Score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse1 = mean_squared_error(actual_day1, predictions[:, 0])\n",
    "mse2 = mean_squared_error(actual_day2, predictions[:, 1])\n",
    "mse3 = mean_squared_error(actual_day3, predictions[:, 2])\n",
    "r2_1 = r2_score(actual_day1, predictions[:, 0])\n",
    "r2_2 = r2_score(actual_day2, predictions[:, 1])\n",
    "r2_3 = r2_score(actual_day3, predictions[:, 2])\n",
    "print(\"Mean Squared Error1:\", mse1)\n",
    "print(\"Mean Squared Error2:\", mse2)\n",
    "print(\"Mean Squared Error3:\", mse3)\n",
    "print(\"R2 Score 1:\", r2_1)\n",
    "print(\"R2 Score 2:\", r2_2)\n",
    "print(\"R2 Score 3:\", r2_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing\n",
    "Nun können wir die Daten mit Hilfe von Matplotlib visualisieren. Wir haben uns dafür entschieden alle drei Tagesvorhersagen als jeweils einen Subplot dazustellen um direkt vergleichen zu können wie sich die Qulität der Predictions verändert, je weiter man in die Zukunft versucht zu schauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_pred_day1 = dates_test.shift(-1)  # Exclude the last 3 dates for the 1st day prediction\n",
    "dates_pred_day2 = dates_test.shift(-2) # Shift for the 2nd day prediction\n",
    "dates_pred_day3 = dates_test.shift(-3) # Shift for the 3rd day prediction\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "\n",
    "# Plot for Day 1 Prediction\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(dates_test, actual_day1, label='Actual', color='blue')\n",
    "plt.plot(dates_pred_day1, predictions[:, 0], label='Predicted Day 1', color='red', alpha=0.7)\n",
    "# plt.title('Day 1 Temperature Prediction')\n",
    "plt.title('Day 1 Rainfall Prediction')\n",
    "plt.xlabel('Date')\n",
    "# plt.ylabel('Temperature')\n",
    "plt.ylabel('Rainfall')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Day 2 Prediction\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(dates_test, actual_day2, label='Actual', color='blue')\n",
    "plt.plot(dates_pred_day2, predictions[:, 1], label='Predicted Day 2', color='orange', alpha=0.7)\n",
    "# plt.title('Day 2 Temperature Prediction')\n",
    "plt.title('Day 2 Rainfall Prediction')\n",
    "plt.xlabel('Date')\n",
    "# plt.ylabel('Temperature')\n",
    "plt.ylabel('Rainfall')\n",
    "plt.legend()\n",
    "\n",
    "# Plot for Day 3 Prediction\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(dates_test, actual_day3, label='Actual', color='blue')\n",
    "plt.plot(dates_pred_day3, predictions[:, 2], label='Predicted Day 3', color='green', alpha=0.7)\n",
    "# plt.title('Day 3 Temperature Prediction')\n",
    "plt.title('Day 3 Rainfall Prediction')\n",
    "plt.xlabel('Date')\n",
    "# plt.ylabel('Temperature')\n",
    "plt.ylabel('Rainfall')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
