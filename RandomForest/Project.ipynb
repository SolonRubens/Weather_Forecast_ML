{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce49e81f-edc8-47ec-8fe3-524398321670",
   "metadata": {},
   "source": [
    "# Wettervorhersage mit Decision Tree Algorithmen\n",
    "---\n",
    "Das Ziel dieses Projekts besteht darin, die Niederschlags- und Lufttemperaturwerte drei Tage in die Zukunft mithilfe von Maschinelerning-Algorithmen, hier insbesondere dem Decision Tree, vorherzusagen. Die zugrundeliegenden Daten stammen aus drei verschiedenen Wetterstationen und werden in drei separaten CSV-Dateien präsentiert.\n",
    "\n",
    "## Datenstruktur:\n",
    "\n",
    "Jede Zeile in den CSV-Dateien repräsentiert einen Tag, wobei verschiedene meteorologische Instrumente unterschiedliche Parameter aufzeichnen. Die verfügbaren Spalten für die Vorhersage umfassen die folgenden Features:\n",
    "- `DATE`\n",
    "- `MESS_DATUM`\n",
    "- `QUALITAETS_NIVEAU`\n",
    "- `LUFTTEMPERATUR`\n",
    "- `DAMPFDRUCK`\n",
    "- `BEDECKUNGSGRAD`\n",
    "- `LUFTDRUCK_STATIONSHOEHE`\n",
    "- `REL_FEUCHTE`\n",
    "- `WINDGESCHWINDIGKEIT`\n",
    "- `LUFTTEMPERATUR_MAXIMUM`\n",
    "- `LUFTTEMPERATUR_MINIMUM`\n",
    "- `LUFTTEMP_AM_ERDB_MINIMUM`\n",
    "- `WINDSPITZE_MAXIMUM`\n",
    "- `NIEDERSCHLAGSHOEHE`\n",
    "- `NIEDERSCHLAGSHOEHE_IND`\n",
    "- `SONNENSCHEINDAUER`\n",
    "- `SCHNEEHOEHE`\n",
    "\n",
    "Es ist wichtig zu beachten, dass fehlende Daten mit dem Wert -999 gekennzeichnet sind. Weiterhin ist zu beachten, dass für jede Wetterstation nicht gleich viele Tage vorhanden sind.\n",
    "\n",
    "## Herangehensweise:\n",
    "\n",
    "Die Vorhersagen werden mithilfe von Decision Tree Algorithmen, speziell dem Random Forest, durchgeführt. Der Random Forest ist eine Ensemble-Methodik, die mehrere Entscheidungsbäume kombiniert, um genauere und robustere Vorhersagen zu erzielen.\n",
    "\n",
    "## Schritte im Projekt:\n",
    "\n",
    "1. Datenimport und -bereinigung:\n",
    "   - Import der Daten aus den drei CSV-Dateien.\n",
    "   - Identifizierung und Handhabung von fehlenden Daten (-999).\n",
    "\n",
    "2. Feature-Engineering:\n",
    "   - Auswahl der relevanten Merkmale (Spalten) für die Vorhersage.\n",
    "   - Mögliche Transformationen oder Skalierungen der Daten für eine bessere Modellleistung.\n",
    "\n",
    "3. Trainieren des Random Forest und verwandter Modelle:\n",
    "   - Aufteilung der Daten in Trainings- und Testsets.\n",
    "   - Identifizierung der besten Parameter für das Random Forest-Modell und Implementierung dieser.\n",
    "   - Identifizierung der besten Parameter für das Bagging-Modell und Implementierung dieser.\n",
    "   - Identifizierung der besten Parameter für das Boosting-Modell und Implementierung dieser.\n",
    "\n",
    "4. Vorhersagen:\n",
    "   - Verwendung des trainierten Modells, um die Niederschlags- und Lufttemperaturwerte für die nächsten drei Tage vorherzusagen.\n",
    "\n",
    "5. Modellbewertung:\n",
    "   - Bewertung der Vorhersagegenauigkeit anhand von Metriken wie MSE oder R-squared für die einzelnen Modelle.\n",
    "   - Bewertung der Verwendbarkeit.\n",
    "   - Bewertung der verwendeten Features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec0926b-fdc8-4c25-81e6-017e9284d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der Bibliotheken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn import neighbors\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import scipy as sc\n",
    "import math as ma\n",
    "from scipy import linalg, optimize, constants, interpolate, special, stats\n",
    "from math import exp, pow, sqrt, log\n",
    "\n",
    "import seaborn as sns #spezielle Graphikdarstellungen\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ff6d6-08d5-4536-9d2e-dae94e05a952",
   "metadata": {},
   "source": [
    "## Daten Importieren\n",
    "Zunächst werden die Daten von den drei Standorten geladen und die Sapltennamen werden so angepasst, dass die Leerzeichen am anfang und ende entfernt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5501c6-9bd6-473d-9fab-e72dc46ab33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "straubingDaten = pd.read_csv(\"../data/Straubing.csv\")\n",
    "arberDaten = pd.read_csv(\"../data/Arber.csv\")\n",
    "schorndorfDaten = pd.read_csv(\"../data/Schorndorf.csv\")\n",
    "\n",
    "# Spaltennamen trimmen\n",
    "straubingDaten.columns = straubingDaten.columns.str.strip()\n",
    "arberDaten.columns = arberDaten.columns.str.strip()\n",
    "schorndorfDaten.columns = schorndorfDaten.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb9b8b6-89af-4bc1-b508-b0ce2bc785e2",
   "metadata": {},
   "source": [
    "## Daten Verarbeiten\n",
    "\n",
    "Um die Daten zu verwenden Müssen Folgende anpassungen gemacht werden:\n",
    "1. Damit aus den Tagen ein Featcherraum wird müssen sowohl die Zieldaten als auch die daten die einzubinden sind in eine Zeile zu schreiben.\n",
    "   Konkret heist das, dass die Zukünftigen daten in eine Aktuelle zeile Kopiert wird.\n",
    "   Gemacht wird dass, indem die Zeilen jeweils kopiert und dann verschoben wird.\n",
    "   Dass funktioniert da die Tage hintereinander stehen, ohne dass es lücken gibt.\n",
    "   Auf diese Weise werden nicht nur die Ziel daten verschoben sondern auch tage zuvor.\n",
    "   Die idee dass tage zuvor auch einfluss haben ist, dass potenziell gelernt werden kann ob beispielsweise die Temperatur tendenziell sinkt oder steigt.\n",
    "2. Im nächsten schritt müssen werden die Einzelnen Datensätze gejoint anhand vom Datum.\n",
    "   Damit gibt es mehr Featchers die potenziell verwendet werden können um eine vorhersage zu machen.\n",
    "3. Nach dem Merge muss mit den Fehlenden Daten umgegangen werden.\n",
    "   Dabei werden manche Featchers interpoliert.\n",
    "   Für den rest wird entschieden, dass falls für einen zu vohersagenden tag, daten nicht vorhanden sind, dieser dann gelöscht wird.\n",
    "4. Zuletzt werden die einzelnen Featchers aufgeteilt in Ziel und Lern daten für Temperatur und Niederschlag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0821b4a-0451-4bb3-b033-1fb6be1e19c1",
   "metadata": {},
   "source": [
    "Die folgende Funktion `filter_columns` wird verwendet um die einzelnen Featchers zu Kopieren und zu verschieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1310a897-1ae1-47bd-8a4a-3d3861aca586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(df, columns, variants=3, extras=None, selected_pass=None, timelag=0):\n",
    "    if extras is None:\n",
    "        extras = []\n",
    "    if selected_pass is None:\n",
    "        selected_pass = []\n",
    "    \n",
    "    # suffix für die spalten hinzufügen\n",
    "    df = df.rename(columns=lambda x: f'{x}_0' if x not in selected_pass else x)\n",
    "\n",
    "    # spalten kopieren und schiften um 1\n",
    "    for v in range(1, variants + 1):\n",
    "        for i in range(len(columns)):\n",
    "            df[f'{columns[i]}_{v}'] = df[f'{columns[i]}_{v-1}'].shift(-1, fill_value=-999)\n",
    "            if columns[i] in extras and v == variants:\n",
    "                df[f'{columns[i]}_{v+1}'] = df[f'{columns[i]}_{v}'].shift(-1-timelag, fill_value=-999)\n",
    "\n",
    "    # Alles wieder zusammensetzen\n",
    "    columns_with_variants = [f'{col}_{v}' for col in columns for v in range(0, variants + 1)] + [f'{col}_{variants+1}' for col in extras]\n",
    "    df_filtered = df[selected_pass + columns_with_variants]\n",
    "\n",
    "    # \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd85bdd-7502-42ef-9093-b07a544b3289",
   "metadata": {},
   "source": [
    "Die Funktion `replace_missing_values` wird dafür verwendet, um die Daten linear zu interpolieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07d420f-bd68-4f36-99b8-3832f73f6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_values(value, df, row, col):\n",
    "    if value != -999:\n",
    "        return value\n",
    "\n",
    "    def replacement_logic(column, row, col):\n",
    "        nonlocal value\n",
    "\n",
    "        distances = np.abs(np.arange(len(column)) - row)\n",
    "\n",
    "        non_missing_values = column[column != -999]\n",
    "\n",
    "        if non_missing_values.size > 0:\n",
    "            weights = np.exp(-0.1 * distances[:len(non_missing_values)])\n",
    "            weighted_mean = np.sum(non_missing_values * weights) / np.sum(weights)\n",
    "            return weighted_mean\n",
    "\n",
    "        return -999\n",
    "\n",
    "    return replacement_logic(df[col].values, row, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49791c-78aa-4d1d-9ed5-0f5b41aed0d6",
   "metadata": {},
   "source": [
    "- `selected_columns`: Eine Liste von Spalten, die für die zeitabhängigen Varianten verwendet werden sollen, einschließlich der Zielspalte.\n",
    "- `interpolation`: Eine Liste an Featchers die interpoliert werden sollen.\n",
    "- `selected_result`: Eine Liste der Zielspalten.\n",
    "- `selected_pass`: Eine Liste an Spalten die keinen lediglich zum joinen von den drei datensätzen ist, diese Spalten werden später wieder rausgenommen.\n",
    "- `vergangenheit`: Der Vergangenheitswert gibt an wie weit in die vergangenheit die Daten mitgenommen wird.\n",
    "- `timelag`: Der Timelag gibt an wie weit in der zukunft die Prognose liegen muss, abhängig vom neuesten Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e154981-19a4-4b6b-a037-ed532535e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['LUFTTEMPERATUR', 'DAMPFDRUCK', 'NIEDERSCHLAGSHOEHE']\n",
    "interpolation = ['LUFTTEMPERATUR']\n",
    "selected_result = ['LUFTTEMPERATUR', 'NIEDERSCHLAGSHOEHE']\n",
    "selected_pass = ['MESS_DATUM']\n",
    "vergangenheit=2\n",
    "timelag = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b230ae9-9040-46e1-b37a-1e0609ee500e",
   "metadata": {},
   "source": [
    "Die Datensätze werden jetzt anhand der Konfiguration zunächst verschoben und umbenannt um die einzelnen Datensätze später zu joinen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb6560c5-4fa5-4cfd-8499-2146be82af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge des Straubing Datensatzes: 23741\n",
      "Länge des Arber Datensatzes: 12114\n",
      "Länge des Schorndorf Datensatzes: 7305\n"
     ]
    }
   ],
   "source": [
    "# Zeitverschiebungen hinzugefügt\n",
    "straubingDatenFiltered = filter_columns(straubingDaten, selected_columns, variants=vergangenheit, extras=selected_result, selected_pass=selected_pass, timelag=timelag)\n",
    "arberDatenFiltered = filter_columns(arberDaten, selected_columns, variants=vergangenheit, extras=[], selected_pass=selected_pass, timelag=timelag)\n",
    "schorndorfDatenFiltered = filter_columns(schorndorfDaten, selected_columns, variants=vergangenheit, extras=[], selected_pass=selected_pass, timelag=timelag)\n",
    "\n",
    "# Prefixes hinzugefügt\n",
    "straubingDatenFiltered = straubingDatenFiltered.add_prefix('straubing_')\n",
    "arberDatenFiltered = arberDatenFiltered.add_prefix('arber_')\n",
    "schorndorfDatenFiltered = schorndorfDatenFiltered.add_prefix('schorndorf_')\n",
    "\n",
    "# Ausgabe der Länge der einzelnen Datensätze\n",
    "print(f'Länge des Straubing Datensatzes: {len(straubingDatenFiltered)}')\n",
    "print(f'Länge des Arber Datensatzes: {len(arberDatenFiltered)}')\n",
    "print(f'Länge des Schorndorf Datensatzes: {len(schorndorfDatenFiltered)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9add725-0a19-4ea9-aeca-c7df9b118c07",
   "metadata": {},
   "source": [
    "Jetzt werden die Einzelnen Daten gemerget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ddb8ad5-c342-4f88-a6d8-6c2a0bb73f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging der Einzelnen Datensätze zu einem neuen (allDatenFiltered)\n",
    "allDatenFiltered = pd.merge(straubingDatenFiltered, arberDatenFiltered, how='outer', left_on='straubing_MESS_DATUM', right_on='arber_MESS_DATUM').merge(schorndorfDatenFiltered, how='outer', left_on='straubing_MESS_DATUM', right_on='schorndorf_MESS_DATUM')\n",
    "allDatenFiltered = allDatenFiltered.fillna(-999)\n",
    "allDatenFiltered = allDatenFiltered.drop(columns=['straubing_MESS_DATUM', 'arber_MESS_DATUM', 'schorndorf_MESS_DATUM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f82fce-41a7-4844-aa45-c90edd31055d",
   "metadata": {},
   "source": [
    "Jetzt werden die Fehlenden Daten linear interpoliert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25676440-8bda-4830-ad73-4b2296a806ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(allDatenFiltered.shape[0]):\n",
    "    for col in [f'{prefix}{name}_{suffix}' for name in interpolation for prefix in ['straubing_', 'arber_', 'schorndorf_'] for suffix in range(0, vergangenheit + (1 if (name in selected_result) else 0), 1)]:\n",
    "        allDatenFiltered[col].iloc[row] = replace_missing_values(allDatenFiltered[col].iloc[row], allDatenFiltered, row, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd695b0-3304-4392-be42-df92b333f3fd",
   "metadata": {},
   "source": [
    "Um einen Überblick zu bekommen, in welcher spalte wie viele daten fehlen, wird die anzahl für jede spalte geplottet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6fe95e-e136-4475-85fe-f6a98e1ba6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl wie viele daten in einer Spalte Fehlen:\n",
      "straubing_LUFTTEMPERATUR_0             0\n",
      "straubing_LUFTTEMPERATUR_1             0\n",
      "straubing_LUFTTEMPERATUR_2             0\n",
      "straubing_DAMPFDRUCK_0              7022\n",
      "straubing_DAMPFDRUCK_1              7023\n",
      "straubing_DAMPFDRUCK_2              7024\n",
      "straubing_NIEDERSCHLAGSHOEHE_0      7000\n",
      "straubing_NIEDERSCHLAGSHOEHE_1      7001\n",
      "straubing_NIEDERSCHLAGSHOEHE_2      7002\n",
      "straubing_LUFTTEMPERATUR_3          7004\n",
      "straubing_NIEDERSCHLAGSHOEHE_3      7004\n",
      "arber_LUFTTEMPERATUR_0                 0\n",
      "arber_LUFTTEMPERATUR_1                 0\n",
      "arber_LUFTTEMPERATUR_2                 0\n",
      "arber_DAMPFDRUCK_0                 11761\n",
      "arber_DAMPFDRUCK_1                 11762\n",
      "arber_DAMPFDRUCK_2                 11763\n",
      "arber_NIEDERSCHLAGSHOEHE_0         11748\n",
      "arber_NIEDERSCHLAGSHOEHE_1         11749\n",
      "arber_NIEDERSCHLAGSHOEHE_2         11750\n",
      "schorndorf_LUFTTEMPERATUR_0            0\n",
      "schorndorf_LUFTTEMPERATUR_1            0\n",
      "schorndorf_LUFTTEMPERATUR_2            0\n",
      "schorndorf_DAMPFDRUCK_0            16687\n",
      "schorndorf_DAMPFDRUCK_1            16687\n",
      "schorndorf_DAMPFDRUCK_2            16687\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_0    16678\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_1    16678\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_2    16678\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(f'Anzahl wie viele daten in einer Spalte Fehlen:\\n{(allDatenFiltered == -999).sum()}')\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c628c-d578-4505-b1ef-102cdd226fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "allDatenFilteredLen = len(allDatenFiltered)\n",
    "allDatenFiltered = allDatenFiltered[~allDatenFiltered.isin([-999]).any(axis=1)].sample(frac=1).reset_index(drop=True)\n",
    "print('#################')\n",
    "print(f'Gesamte Datengröße: {allDatenFilteredLen}\\nVerlorene Daten durch die Filterung: {allDatenFilteredLen - len(allDatenFiltered)}\\nFinale Datengröße: {len(allDatenFiltered)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b72ebe-9a9f-47f0-bbf6-3d28deff1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperation von Featchers und Zieldaten (x, y)\n",
    "Y_Daten_Temp = allDatenFiltered.loc[:, [f'straubing_LUFTTEMPERATUR_{vergangenheit+1}']].copy()\n",
    "X_Daten_Temp = allDatenFiltered.drop(columns=[f'straubing_LUFTTEMPERATUR_{vergangenheit+1}']).drop(columns=[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit+1}'])\n",
    "\n",
    "Y_Daten_Reg = allDatenFiltered.loc[:, [f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit+1}']].copy()\n",
    "X_Daten_Reg = allDatenFiltered.drop(columns=[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit+1}']).drop(columns=[f'straubing_LUFTTEMPERATUR_{vergangenheit+1}'])\n",
    "\n",
    "X_Temp = X_Daten_Temp.values\n",
    "Y_Temp = Y_Daten_Temp.values.flatten()\n",
    "\n",
    "X_Reg = X_Daten_Reg.values\n",
    "Y_Reg = Y_Daten_Reg.values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662e981-bf57-480a-92ad-a9573e1e69a7",
   "metadata": {},
   "source": [
    "## Parameter für den Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae0c23-03d9-4b9b-b6ec-8ebf6ee499d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der benötigten Bibliotheken\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d8430-217d-4033-adf8-e592a5f5c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, n_estimators, max_depth, test_size):\n",
    "    # Train Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size)\n",
    "\n",
    "    # Initialisieren des Random Forest Regressors\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=n_estimators, n_jobs=-1, max_depth=max_depth)\n",
    "    \n",
    "    # Trainieren des Regressors mit den Trainingsdaten\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Vorhersagen auf den Testdaten\n",
    "    y_pred_test = rf_regressor.predict(X_test)\n",
    "    y_pred_train = rf_regressor.predict(X_train)\n",
    "\n",
    "    return (mean_squared_error(y_test, y_pred_test), mean_squared_error(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd41f4-5bd6-4f39-b0ae-b87e694a1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_range = range(10, 50, 5)\n",
    "depth_range = range(10, 50, 5)\n",
    "split_range = range(5, 50, 5)\n",
    "\n",
    "estimators_mse_test_Temp = []\n",
    "estimators_mse_train_Temp = []\n",
    "depth_mse_test_Temp = []\n",
    "depth_mse_train_Temp = []\n",
    "split_mse_test_Temp = []\n",
    "split_mse_train_Temp = []\n",
    "estimators_mse_test_Reg = []\n",
    "estimators_mse_train_Reg = []\n",
    "depth_mse_test_Reg = []\n",
    "depth_mse_train_Reg = []\n",
    "split_mse_test_Reg = []\n",
    "split_mse_train_Reg = []\n",
    "\n",
    "for est in estimators_range:\n",
    "    mse_test, mse_train = train(X_Temp, Y_Temp, est, 20, 0.2)\n",
    "    estimators_mse_test_Temp.append(mse_test)\n",
    "    estimators_mse_train_Temp.append(mse_train)\n",
    "    \n",
    "for depth in depth_range:\n",
    "    mse_test, mse_train = train(X_Temp, Y_Temp, 50, depth, 0.2)\n",
    "    depth_mse_test_Temp.append(mse_test)\n",
    "    depth_mse_train_Temp.append(mse_train)\n",
    "    \n",
    "for split in split_range:\n",
    "    mse_test, mse_train = train(X_Temp, Y_Temp, 50, 20, split/100)\n",
    "    split_mse_test_Temp.append(mse_test)\n",
    "    split_mse_train_Temp.append(mse_train)\n",
    "\n",
    "for est in estimators_range:\n",
    "    mse_test, mse_train = train(X_Reg, Y_Reg, est, 40, 0.2)\n",
    "    estimators_mse_test_Reg.append(mse_test)\n",
    "    estimators_mse_train_Reg.append(mse_train)\n",
    "    \n",
    "for depth in depth_range:\n",
    "    mse_test, mse_train = train(X_Reg, Y_Reg, 50, depth, 0.2)\n",
    "    depth_mse_test_Reg.append(mse_test)\n",
    "    depth_mse_train_Reg.append(mse_train)\n",
    "    \n",
    "for split in split_range:\n",
    "    mse_test, mse_train = train(X_Reg, Y_Reg, 50, 40, split/100)\n",
    "    split_mse_test_Reg.append(mse_test)\n",
    "    split_mse_train_Reg.append(mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676ec35-3bcb-4d47-8dd4-052d41809b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen von Subplots (2 Zeilen, 3 Spalten)\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Forest Trees\n",
    "axs[0, 0].plot(estimators_range, estimators_mse_test_Temp, marker='o', label='Test MSE')\n",
    "axs[0, 0].plot(estimators_range, estimators_mse_train_Temp, marker='o', label='Train MSE')\n",
    "axs[0, 0].set_ylabel('MSE (Mean Squared Error)')\n",
    "axs[0, 0].set_xlabel('Trees')\n",
    "axs[0, 0].set_title('Forest Trees')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Plot 2: Tree Depth\n",
    "axs[0, 1].plot(depth_range, depth_mse_test_Temp, marker='o', label='Test MSE')\n",
    "axs[0, 1].plot(depth_range, depth_mse_train_Temp, marker='o', label='Train MSE')\n",
    "axs[0, 1].set_ylabel('MSE (Mean Squared Error)')\n",
    "axs[0, 1].set_xlabel('Depth')\n",
    "axs[0, 1].set_title('Tree Depth')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Plot 3: Train Test Split\n",
    "axs[0, 2].plot([split/100 for split in split_range], split_mse_test_Temp, marker='o', label='Test MSE')\n",
    "axs[0, 2].plot([split/100 for split in split_range], split_mse_train_Temp, marker='o', label='Train MSE')\n",
    "axs[0, 2].set_ylabel('MSE (Mean Squared Error)')\n",
    "axs[0, 2].set_xlabel('Split')\n",
    "axs[0, 2].set_title('Train Test Split')\n",
    "axs[0, 2].legend()\n",
    "\n",
    "# Plot 4: Forest Trees\n",
    "axs[1, 0].plot(estimators_range, estimators_mse_test_Reg, marker='o', label='Test MSE')\n",
    "axs[1, 0].plot(estimators_range, estimators_mse_train_Reg, marker='o', label='Train MSE')\n",
    "axs[1, 0].set_ylabel('MSE (Mean Squared Error)')\n",
    "axs[1, 0].set_xlabel('Trees')\n",
    "axs[1, 0].set_title('Forest Trees')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Plot 5: Tree Depth\n",
    "axs[1, 1].plot(depth_range, depth_mse_test_Reg, marker='o', label='Test MSE')\n",
    "axs[1, 1].plot(depth_range, depth_mse_train_Reg, marker='o', label='Train MSE')\n",
    "axs[1, 1].set_ylabel('MSE (Mean Squared Error)')\n",
    "axs[1, 1].set_xlabel('Depth')\n",
    "axs[1, 1].set_title('Tree Depth')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# Plot 6: Train Test Split\n",
    "axs[1, 2].plot([split/100 for split in split_range], split_mse_test_Reg, marker='o', label='Test MSE')\n",
    "axs[1, 2].plot([split/100 for split in split_range], split_mse_train_Reg, marker='o', label='Train MSE')\n",
    "axs[1, 2].set_ylabel('MSE (Mean Squared Error)')\n",
    "axs[1, 2].set_xlabel('Split')\n",
    "axs[1, 2].set_title('Train Test Split')\n",
    "axs[1, 2].legend()\n",
    "\n",
    "# Layout anpassen\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e72920-1b25-46a7-b7b8-61160580fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Train Test split\n",
    "X_train_Temp, X_test_Temp, y_train_Temp, y_test_Temp = train_test_split(X_Temp, Y_Temp, test_size=0.2)\n",
    "X_train_Reg, X_test_Reg, y_train_Reg, y_test_Reg = train_test_split(X_Reg, Y_Reg, test_size=0.2)\n",
    "\n",
    "# Gradient Boosting für die Temperatur-Vorhersagen\n",
    "gb_regressor_Temp = GradientBoostingRegressor(n_estimators=100,  # Anzahl der Bäume im Ensemble\n",
    "                                              learning_rate=0.1,  # Lernrate\n",
    "                                              max_depth=4,  # Maximale Tiefe der Bäume\n",
    "                                              random_state=42)\n",
    "\n",
    "# Trainieren des Gradient Boosting-Modells mit den Trainingsdaten\n",
    "gb_regressor_Temp.fit(X_train_Temp, y_train_Temp)\n",
    "\n",
    "# Vorhersagen auf den Testdaten\n",
    "y_pred_test_Temp = gb_regressor_Temp.predict(X_test_Temp)\n",
    "y_pred_train_Temp = gb_regressor_Temp.predict(X_train_Temp)\n",
    "\n",
    "# Auswertung des Gradient Boosting-Modells für die Temperatur\n",
    "print(\"RMSE Temperatur Vortag: \" + str(mean_squared_error(Y_Daten_Temp, X_Daten_Temp[f'straubing_LUFTTEMPERATUR_{vergangenheit}'])))\n",
    "print(\"RMSE Temperatur Test:   \" + str(mean_squared_error(y_test_Temp, y_pred_test_Temp)))\n",
    "print(\"RMSE Temperatur Train:  \" + str(mean_squared_error(y_train_Temp, y_pred_train_Temp)))\n",
    "\n",
    "# Gradient Boosting für die Niederschlags-Vorhersagen\n",
    "gb_regressor_Reg = GradientBoostingRegressor(n_estimators=100,  # Anzahl der Bäume im Ensemble\n",
    "                                             learning_rate=0.1,  # Lernrate\n",
    "                                             max_depth=4,  # Maximale Tiefe der Bäume\n",
    "                                             random_state=42)\n",
    "\n",
    "# Trainieren des Gradient Boosting-Modells mit den Trainingsdaten\n",
    "gb_regressor_Reg.fit(X_train_Reg, y_train_Reg)\n",
    "\n",
    "# Vorhersagen auf den Testdaten\n",
    "y_pred_test_Reg = gb_regressor_Reg.predict(X_test_Reg)\n",
    "y_pred_train_Reg = gb_regressor_Reg.predict(X_train_Reg)\n",
    "\n",
    "# Auswertung des Gradient Boosting-Modells für den Niederschlag\n",
    "print(\"RMSE Niederschlag Vortag: \" + str(mean_squared_error(Y_Daten_Reg, X_Daten_Reg[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit}'])))\n",
    "print(\"RMSE Niederschlag Test:   \" + str(mean_squared_error(y_test_Reg, y_pred_test_Reg)))\n",
    "print(\"RMSE Niederschlag Train:  \" + str(mean_squared_error(y_train_Reg, y_pred_train_Reg)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2dff5-490d-45d8-b6b5-e36394de8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# Train Test split\n",
    "X_train_Temp, X_test_Temp, y_train_Temp, y_test_Temp = train_test_split(X_Temp, Y_Temp, test_size=0.2)\n",
    "X_train_Reg, X_test_Reg, y_train_Reg, y_test_Reg = train_test_split(X_Reg, Y_Reg, test_size=0.2)\n",
    "\n",
    "# Bagging für die Temperatur-Vorhersagen\n",
    "bagging_regressor_Temp = BaggingRegressor(estimator=RandomForestRegressor(n_estimators=50, n_jobs=-1, max_depth=20),\n",
    "                                          n_estimators=10,  # Anzahl der Modelle im Ensemble\n",
    "                                          random_state=42)\n",
    "\n",
    "# Trainieren des Bagging-Modells mit den Trainingsdaten\n",
    "bagging_regressor_Temp.fit(X_train_Temp, y_train_Temp)\n",
    "\n",
    "# Vorhersagen auf den Testdaten\n",
    "y_pred_test_Temp = bagging_regressor_Temp.predict(X_test_Temp)\n",
    "y_pred_train_Temp = bagging_regressor_Temp.predict(X_train_Temp)\n",
    "\n",
    "# Auswertung des Bagging-Modells für die Temperatur\n",
    "print(\"RMSE Temperatur Vortag: \" + str(mean_squared_error(Y_Daten_Temp, X_Daten_Temp[f'straubing_LUFTTEMPERATUR_{vergangenheit}'])))\n",
    "print(\"RMSE Temperatur Test:   \" + str(mean_squared_error(y_test_Temp, y_pred_test_Temp)))\n",
    "print(\"RMSE Temperatur Train:  \" + str(mean_squared_error(y_train_Temp, y_pred_train_Temp)))\n",
    "\n",
    "# Bagging für die Niederschlags-Vorhersagen\n",
    "bagging_regressor_Reg = BaggingRegressor(estimator=RandomForestRegressor(n_estimators=50, n_jobs=-1, max_depth=40),\n",
    "                                         n_estimators=10,  # Anzahl der Modelle im Ensemble\n",
    "                                         random_state=42)\n",
    "\n",
    "# Trainieren des Bagging-Modells mit den Trainingsdaten\n",
    "bagging_regressor_Reg.fit(X_train_Reg, y_train_Reg)\n",
    "\n",
    "# Vorhersagen auf den Testdaten\n",
    "y_pred_test_Reg = bagging_regressor_Reg.predict(X_test_Reg)\n",
    "y_pred_train_Reg = bagging_regressor_Reg.predict(X_train_Reg)\n",
    "\n",
    "# Auswertung des Bagging-Modells für den Niederschlag\n",
    "print(\"RMSE Niederschlag Vortag: \" + str(mean_squared_error(Y_Daten_Reg, X_Daten_Reg[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit}'])))\n",
    "print(\"RMSE Niederschlag Test:   \" + str(mean_squared_error(y_test_Reg, y_pred_test_Reg)))\n",
    "print(\"RMSE Niederschlag Train:  \" + str(mean_squared_error(y_train_Reg, y_pred_train_Reg)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506f631-044f-4866-a25a-74a844da1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Test split\n",
    "X_train_Temp, X_test_Temp, y_train_Temp, y_test_Temp = train_test_split(X_Temp, Y_Temp, test_size=0.2)\n",
    "X_train_Reg, X_test_Reg, y_train_Reg, y_test_Reg = train_test_split(X_Reg, Y_Reg, test_size=0.2)\n",
    "\n",
    "# Initialisieren des Random Forest Regressors\n",
    "rf_regressor_Temp = RandomForestRegressor(n_estimators=50, n_jobs=-1, max_depth=20)\n",
    "rf_regressor_Reg = RandomForestRegressor(n_estimators=50, n_jobs=-1, max_depth=40)\n",
    "    \n",
    "# Trainieren des Regressors mit den Trainingsdaten\n",
    "rf_regressor_Temp.fit(X_train_Temp, y_train_Temp)\n",
    "rf_regressor_Reg.fit(X_train_Reg, y_train_Reg)\n",
    "    \n",
    "# Vorhersagen auf den Testdaten\n",
    "y_pred_test_Temp = rf_regressor_Temp.predict(X_test_Temp)\n",
    "y_pred_train_Temp = rf_regressor_Temp.predict(X_train_Temp)\n",
    "y_pred_test_Reg = rf_regressor_Reg.predict(X_test_Reg)\n",
    "y_pred_train_Reg = rf_regressor_Reg.predict(X_train_Reg)\n",
    "    \n",
    "# Auswertung des Modells (z.B. Mean Squared Error für Regression)\n",
    "print(\"RMSE Temperatur Vortag:\" + str(mean_squared_error(Y_Daten_Temp, X_Daten_Temp[f'straubing_LUFTTEMPERATUR_{vergangenheit}'])))\n",
    "print(\"RMSE Temperatur Test:  \" + str(mean_squared_error(y_test_Temp, y_pred_test_Temp)))\n",
    "print(\"RMSE Temperatur Train: \" + str(mean_squared_error(y_train_Temp, y_pred_train_Temp)))\n",
    "print(\"RMSE Niderschlag Vortag:\" + str(mean_squared_error(Y_Daten_Reg, X_Daten_Reg[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit}'])))\n",
    "print(\"RMSE Niderschlag Test:  \" + str(mean_squared_error(y_test_Reg, y_pred_test_Reg)))\n",
    "print(\"RMSE Niderschlag Train: \" + str(mean_squared_error(y_train_Reg, y_pred_train_Reg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138eef8-70d5-494f-a8f1-8773a0685ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplots nebeneinander darstellen\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Scatterplot im Traindatensatz\n",
    "axs[0, 0].scatter(Y_Daten_Temp, X_Daten_Temp[f'straubing_LUFTTEMPERATUR_{vergangenheit}'], alpha=0.5, label='Verhältnist zur vortags Temperatur')\n",
    "axs[0, 0].scatter(y_train_Temp, y_pred_train_Temp, alpha=0.5, label='Vorhersagen')\n",
    "axs[0, 0].plot(axs[0, 0].get_xlim(), axs[0, 0].get_xlim(), color='red', linestyle='--', label='Ideallinie')\n",
    "axs[0, 0].set_xlabel('Tatsächliche Werte')\n",
    "axs[0, 0].set_ylabel('Vorhersagen')\n",
    "axs[0, 0].set_title('Vergleich der tatsächlichen Werte und Vorhersagen im Traindatensatz')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Scatterplot im Testdatensatz\n",
    "axs[0, 1].scatter(Y_Daten_Temp, X_Daten_Temp[f'straubing_LUFTTEMPERATUR_{vergangenheit}'], alpha=0.5, label='Verhältnist zur vortags Temperatur')\n",
    "axs[0, 1].scatter(y_test_Temp, y_pred_test_Temp, alpha=0.5, label='Vorhersagen')\n",
    "axs[0, 1].plot(axs[0, 1].get_xlim(), axs[0, 1].get_xlim(), color='red', linestyle='--', label='Ideallinie')\n",
    "axs[0, 1].set_xlabel('Tatsächliche Werte')\n",
    "axs[0, 1].set_ylabel('Vorhersagen')\n",
    "axs[0, 1].set_title('Vergleich der tatsächlichen Werte und Vorhersagen im Testdatensatz')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Scatterplot im Traindatensatz\n",
    "axs[1, 0].scatter(Y_Daten_Reg, X_Daten_Reg[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit}'], alpha=0.5, label='Verhältnist zur vortags Temperatur')\n",
    "axs[1, 0].scatter(y_train_Reg, y_pred_train_Reg, alpha=0.5, label='Vorhersagen')\n",
    "axs[1, 0].plot(axs[1, 0].get_xlim(), axs[1, 0].get_xlim(), color='red', linestyle='--', label='Ideallinie')\n",
    "axs[1, 0].set_xlabel('Tatsächliche Werte')\n",
    "axs[1, 0].set_ylabel('Vorhersagen')\n",
    "axs[1, 0].set_title('Vergleich der tatsächlichen Werte und Vorhersagen im Traindatensatz')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Scatterplot im Testdatensatz\n",
    "axs[1, 1].scatter(Y_Daten_Reg, X_Daten_Reg[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit}'], alpha=0.5, label='Verhältnist zur vortags Temperatur')\n",
    "axs[1, 1].scatter(y_test_Reg, y_pred_test_Reg, alpha=0.5, label='Vorhersagen')\n",
    "axs[1, 1].plot(axs[1, 1].get_xlim(), axs[1, 1].get_xlim(), color='red', linestyle='--', label='Ideallinie')\n",
    "axs[1, 1].set_xlabel('Tatsächliche Werte')\n",
    "axs[1, 1].set_ylabel('Vorhersagen')\n",
    "axs[1, 1].set_title('Vergleich der tatsächlichen Werte und Vorhersagen im Testdatensatz')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529f3e6-cb59-470b-80a0-c360830d32e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unterschied der Arrays berechnen\n",
    "runden = 1\n",
    "diff_array_test_Temp = np.abs(np.round(y_test_Temp * runden) / runden - np.round(y_pred_test_Temp * runden) / runden)\n",
    "diff_array_train_Temp = np.abs(np.round(y_train_Temp * runden) / runden - np.round(y_pred_train_Temp * runden) / runden)\n",
    "diff_array_test_Reg = np.abs(np.round(y_test_Reg * runden) / runden - np.round(y_pred_test_Reg * runden) / runden)\n",
    "diff_array_train_Reg = np.abs(np.round(y_train_Reg * runden) / runden - np.round(y_pred_train_Reg * runden) / runden)\n",
    "\n",
    "unique_elements_test_Temp, counts_test_Temp = np.unique(diff_array_test_Temp, return_counts=True)\n",
    "unique_elements_train_Temp, counts_train_Temp = np.unique(diff_array_train_Temp, return_counts=True)\n",
    "unique_elements_test_Reg, counts_test_Reg = np.unique(diff_array_test_Reg, return_counts=True)\n",
    "unique_elements_train_Reg, counts_train_Reg = np.unique(diff_array_train_Reg, return_counts=True)\n",
    "\n",
    "# Werte-Bereich mit dem gewünschten Abstand erstellen\n",
    "max_value_Temp = max(max(unique_elements_test_Temp), max(unique_elements_train_Temp))\n",
    "max_value_Reg = max(max(unique_elements_test_Reg), max(unique_elements_train_Reg))\n",
    "def expand(min, max, step, val, count):\n",
    "    all_values = np.arange(min, max + step, step)\n",
    "    # Index-Array für vorhandene Werte erstellen\n",
    "    existing_values_idx = np.isin(all_values, val)\n",
    "    # Fehlende Werte mit Häufigkeit 0 einfügen\n",
    "    all_frequencies = np.zeros_like(all_values, dtype=int)\n",
    "    all_frequencies[existing_values_idx] = count\n",
    "    return (all_values, all_frequencies)\n",
    "\n",
    "unique_elements_test_Temp, counts_test_Temp = expand(0, max_value_Temp, 1/runden, unique_elements_test_Temp, counts_test_Temp)\n",
    "unique_elements_train_Temp, counts_train_Temp = expand(0, max_value_Temp, 1/runden, unique_elements_train_Temp, counts_train_Temp)\n",
    "unique_elements_test_Reg, counts_test_Reg = expand(0, max_value_Reg, 1/runden, unique_elements_test_Reg, counts_test_Reg)\n",
    "unique_elements_train_Reg, counts_train_Reg = expand(0, max_value_Reg, 1/runden, unique_elements_train_Reg, counts_train_Reg)\n",
    "\n",
    "\n",
    "# Verteilung plotten als Bar Plot\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(20, 14))\n",
    "\n",
    "# Breite der Bars\n",
    "bar_width = 0.5\n",
    "\n",
    "# Positionen der x-Ticks für die beiden Gruppen\n",
    "x_ticks_test_Temp = np.arange(len(unique_elements_test_Temp))\n",
    "x_ticks_train_Temp = np.arange(len(unique_elements_train_Temp))\n",
    "x_ticks_test_Reg = np.arange(len(unique_elements_test_Reg))\n",
    "x_ticks_train_Reg = np.arange(len(unique_elements_train_Reg))\n",
    "\n",
    "# Plot für Testdaten\n",
    "bar_test_Temp = ax1.bar(x_ticks_test_Temp, counts_test_Temp, width=bar_width, label=\"Test Daten\")\n",
    "ax1.set_xlabel('Grad')\n",
    "ax1.set_ylabel('Häufigkeit')\n",
    "ax1.set_xticks(x_ticks_test_Temp)\n",
    "ax1.set_xticklabels(unique_elements_test_Temp)\n",
    "ax1.legend()\n",
    "ax1.set_title('Verteilung des absoluten Fehlers der Testdaten')\n",
    "\n",
    "# Text über den Bars anzeigen\n",
    "for bar, counts in zip(bar_test_Temp, counts_test_Temp):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width() / 2, height, f'{counts}', \n",
    "            ha='center', va='bottom')\n",
    "\n",
    "# Plot für Trainingsdaten\n",
    "bar_train_Temp = ax2.bar(x_ticks_train_Temp, counts_train_Temp, width=bar_width, label=\"Trainings Daten\")\n",
    "ax2.set_xlabel('Grad')\n",
    "ax2.set_ylabel('Häufigkeit')\n",
    "ax2.set_xticks(x_ticks_train_Temp)\n",
    "ax2.set_xticklabels(unique_elements_train_Temp)\n",
    "ax2.legend()\n",
    "ax2.set_title('Verteilung des absoluten Fehlers der Trainingsdaten')\n",
    "\n",
    "# Text über den Bars anzeigen\n",
    "for bar, counts in zip(bar_train_Temp, counts_train_Temp):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2, height, f'{counts}', \n",
    "            ha='center', va='bottom')\n",
    "\n",
    "\n",
    "# Plot für Testdaten\n",
    "bar_test_Reg = ax3.bar(x_ticks_test_Reg, counts_test_Reg, width=bar_width, label=\"Test Daten\")\n",
    "ax3.set_xlabel('Grad')\n",
    "ax3.set_ylabel('Häufigkeit')\n",
    "ax3.set_xticks(x_ticks_test_Reg)\n",
    "ax3.set_xticklabels(unique_elements_test_Reg)\n",
    "ax3.legend()\n",
    "ax3.set_title('Verteilung des absoluten Fehlers der Testdaten')\n",
    "\n",
    "# Text über den Bars anzeigen\n",
    "for bar, counts in zip(bar_test_Reg, counts_test_Reg):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width() / 2, height, f'{counts}', \n",
    "            ha='center', va='bottom')\n",
    "\n",
    "# Plot für Trainingsdaten\n",
    "bar_train_Reg = ax4.bar(x_ticks_train_Reg, counts_train_Reg, width=bar_width, label=\"Trainings Daten\")\n",
    "ax4.set_xlabel('Grad')\n",
    "ax4.set_ylabel('Häufigkeit')\n",
    "ax4.set_xticks(x_ticks_train_Reg)\n",
    "ax4.set_xticklabels(unique_elements_train_Reg)\n",
    "ax4.legend()\n",
    "ax4.set_title('Verteilung des absoluten Fehlers der Trainingsdaten')\n",
    "\n",
    "# Text über den Bars anzeigen\n",
    "for bar, counts in zip(bar_train_Reg, counts_train_Reg):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width() / 2, height, f'{counts}', \n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3e891-aa79-4fbe-9cd5-598a9d7102e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahieren der Importance der Trees\n",
    "importances_Temp = np.round(rf_regressor_Temp.feature_importances_*100, 2)\n",
    "std_Temp = np.round(np.std([tree.feature_importances_ for tree in rf_regressor_Temp.estimators_], axis=0)*100, 2)\n",
    "importances_Reg = np.round(rf_regressor_Reg.feature_importances_*100, 2)\n",
    "std_Reg = np.round(np.std([tree.feature_importances_ for tree in rf_regressor_Reg.estimators_], axis=0)*100, 2)\n",
    "\n",
    "# Indizes der sortierten Importance-Werte\n",
    "sorted_indices_Temp = np.argsort(importances_Temp)\n",
    "sorted_indices_Reg = np.argsort(importances_Reg)\n",
    "\n",
    "# Sortieren der Importance und Standardabweichung\n",
    "importances_Temp = importances_Temp[sorted_indices_Temp]\n",
    "std_Temp = std_Temp[sorted_indices_Temp]\n",
    "importances_Reg = importances_Reg[sorted_indices_Reg]\n",
    "std_Reg = std_Reg[sorted_indices_Reg]\n",
    "\n",
    "forest_importances_Temp = pd.Series(importances_Temp, index=X_Daten_Temp.columns[sorted_indices_Temp])\n",
    "forest_importances_Reg = pd.Series(importances_Reg, index=X_Daten_Reg.columns[sorted_indices_Reg])\n",
    "\n",
    "# Breite des Bildes anpassen\n",
    "fig, ax = plt.subplots(figsize=(8, 30))\n",
    "\n",
    "# Balkenplot mit Fehlerbalken\n",
    "forest_importances_Temp.plot.barh(xerr=std_Temp, ax=ax, capsize=4)\n",
    "\n",
    "# Titel und Achsenbeschriftungen\n",
    "ax.set_title(\"Feature importances with uncertainty\")\n",
    "ax.set_xlabel(\"Mean decrease in impurity\")\n",
    "\n",
    "# Logarithmische Skala für die x-Achse\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Werte neben den Balken anzeigen\n",
    "for i, (feature, importance, uncertainty) in enumerate(zip(forest_importances_Temp.index, forest_importances_Temp.values, std_Temp)):\n",
    "    ax.text(importance + uncertainty + 0.1, i, f'{importance:.2f}% ±{uncertainty:.2f}%', ha='left', va='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Breite des Bildes anpassen\n",
    "fig, ax = plt.subplots(figsize=(8, 30))\n",
    "\n",
    "# Balkenplot mit Fehlerbalken\n",
    "forest_importances_Reg.plot.barh(xerr=std_Reg, ax=ax, capsize=4)\n",
    "\n",
    "# Titel und Achsenbeschriftungen\n",
    "ax.set_title(\"Feature importances with uncertainty\")\n",
    "ax.set_xlabel(\"Mean decrease in impurity\")\n",
    "\n",
    "# Logarithmische Skala für die x-Achse\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Werte neben den Balken anzeigen\n",
    "for i, (feature, importance, uncertainty) in enumerate(zip(forest_importances_Reg.index, forest_importances_Reg.values, std_Reg)):\n",
    "    ax.text(importance + uncertainty + 0.1, i, f'{importance:.2f}% ±{uncertainty:.2f}%', ha='left', va='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4793e4-cbd4-462a-b179-851119e6a4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6f219-e467-4f28-8c93-80342aac4d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
