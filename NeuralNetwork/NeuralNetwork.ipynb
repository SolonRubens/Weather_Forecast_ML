{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wettervorhersagen mit Hilfe eines Neuronalen Netzes\n",
    "\n",
    "Diese Datei ist der \"Neuronale Netz\"-Teil im Vergleich zwischen Linear Regression, Random Forest und Neuronalen Netzwerken im Bezug darauf, welches Modell besser in der Lage dazu ist Wetterdaten (Temperatur und Niederschlag) mit Hilfe von Datensätzen aus drei Wetterstationen, 3 Tage in die Zukunft vorherzusagen. Die zugrundeliegenden Daten liegen in CSV Format vor.\n",
    "\n",
    "## Datenstruktur\n",
    "Jede Zeile in den vorliegenden CSV Dateien repräsentieren (Wetter)daten eines Tags in chronologischer Reihenfolge. Die verfügbaren Spalten umfassen die folgenden Features:\n",
    "- `DATE`\n",
    "- `MESS_DATUM`\n",
    "- `QUALITAETS_NIVEAU`\n",
    "- `LUFTTEMPERATUR`\n",
    "- `DAMPFDRUCK`\n",
    "- `BEDECKUNGSGRAD`\n",
    "- `LUFTDRUCK_STATIONSHOEHE`\n",
    "- `REL_FEUCHTE`\n",
    "- `WINDGESCHWINDIGKEIT`\n",
    "- `LUFTTEMPERATUR_MAXIMUM`\n",
    "- `LUFTTEMPERATUR_MINIMUM`\n",
    "- `LUFTTEMP_AM_ERDB_MINIMUM`\n",
    "- `WINDSPITZE_MAXIMUM`\n",
    "- `NIEDERSCHLAGSHOEHE`\n",
    "- `NIEDERSCHLAGSHOEHE_IND`\n",
    "- `SONNENSCHEINDAUER`\n",
    "- `SCHNEEHOEHE`\n",
    "\n",
    "Es ist dabei wichtig zu beachten, dass fehlende Daten mit dem WErt -999 gekennzeichnet sind. Weiterhin ist zu beachten, dass die Wetterstationen unterschiedliche Daten als Aufzeichnungsbeginn haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der nötigen Bibliotheken\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten importieren\n",
    "Nachdem wir die für unser Programm nötigen Bibliotheken importiert haben müssen wir nun die Wetterdaten einlesen. Zusätzlich werden die Leerzeichen am Anfang und Ende der Spaltennamen entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "arber_data = pd.read_csv(cwd + \"/data/Arber.csv\")\n",
    "schorndorf_data = pd.read_csv(cwd + \"/data/Schorndorf.csv\")\n",
    "straubing_data = pd.read_csv(cwd + \"/data/Straubing.csv\")\n",
    "\n",
    "# Strippen\n",
    "arber_data.columns = arber_data.columns.str.strip()\n",
    "schorndorf_data.columns = schorndorf_data.columns.str.strip()\n",
    "straubing_data.columns = straubing_data.columns.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "Um unsere Daten gezielt weiterverarbeiten zu können splitten wir sie pro Wetterstation in Numeric und Non-Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split numeric and non numeric columns\n",
    "arber_numeric = arber_data.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "arber_non_numeric = arber_data.select_dtypes(exclude=[\"float64\", \"int64\"])\n",
    "\n",
    "schorndorf_numeric = schorndorf_data.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "schorndorf_non_numeric = schorndorf_data.select_dtypes(exclude=[\"float64\", \"int64\"])\n",
    "\n",
    "straubing_numeric = straubing_data.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "straubing_non_numeric = straubing_data.select_dtypes(exclude=[\"float64\", \"int64\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handeling missing values\n",
    "Nachdem wir nun Zugriff auf die numerischen Werte haben können wir uns nun mit den Fehlenden Werten beschäftigen. Da wir einige leere Werte haben füllen wir diese mit dem mean value des columns mit Hilfe eines Imputers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with the mean value only on numeric data\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "arber_numeric_filled = pd.DataFrame(imputer.fit_transform(arber_numeric.replace), columns=arber_numeric.columns)\n",
    "schorndorf_numeric_filled = pd.DataFrame(imputer.fit_transform(schorndorf_numeric), columns=schorndorf_numeric.columns)\n",
    "straubing_numeric_filled = pd.DataFrame(imputer.fit_transform(straubing_numeric), columns=straubing_numeric.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining back together numeric and non-numeric data\n",
    "Nachdem wir die fehlenden Werte mit einem Durchschnittswert gefüllt haben können wir die numerischen und nicht numerischen Daten wieder zusammenführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining back together the numeric and non numeric data\n",
    "arber_data_filled = pd.concat([arber_non_numeric.reset_index(drop=True), arber_numeric_filled.reset_index(drop=True)], axis=1)\n",
    "schorndorf_data_filled = pd.concat([schorndorf_non_numeric.reset_index(drop=True), schorndorf_numeric_filled.reset_index(drop=True)], axis=1)\n",
    "straubing_data_filled = pd.concat([straubing_non_numeric.reset_index(drop=True), straubing_numeric_filled.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features\n",
    "Nach der Rückkombination der Daten suchen wir nun die relevanten Features aus unseren Daten aus. Für das Neuronale Netzwerk haben wir uns für die folgenden Features entschieden:\n",
    "- `DATE`\n",
    "- `LUFTTEMPERATUR`\n",
    "- `DAMPFDRUCK`\n",
    "- `REL_FEUCHTE`\n",
    "- `WINDGESCHWINDIGKEIT`\n",
    "- `LUFTTEMPERATUR_MAXIMUM`\n",
    "- `LUFTTEMPERATUR_MINIMUM`\n",
    "- `LUFTTEMP_AM_ERDB_MINIMUM`\n",
    "- `NIEDERSCHLAGSHOEHE`\n",
    "- `NIEDERSCHLAGSHOEHE_IND`\n",
    "- `SCHNEEHOEHE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant features\n",
    "arber_data_selected = arber_data_filled[[\"DATE\", \"LUFTTEMPERATUR\",  \"REL_FEUCHTE\", \"LUFTTEMPERATUR_MAXIMUM\", \"LUFTTEMPERATUR_MINIMUM\", \"LUFTTEMP_AM_ERDB_MINIMUM\", \"NIEDERSCHLAGSHOEHE\", \"DAMPFDRUCK\", \"NIEDERSCHLAGSHOEHE_IND\", \"SCHNEEHOEHE\"]]\n",
    "schorndorf_data_selected = schorndorf_data_filled[[\"DATE\", \"LUFTTEMPERATUR\", \"REL_FEUCHTE\", \"NIEDERSCHLAGSHOEHE\", \"LUFTTEMPERATUR_MAXIMUM\", \"LUFTTEMPERATUR_MINIMUM\", \"LUFTTEMP_AM_ERDB_MINIMUM\", \"DAMPFDRUCK\", \"NIEDERSCHLAGSHOEHE_IND\", \"SCHNEEHOEHE\"]]\n",
    "straubing_data_selected = straubing_data_filled[[\"DATE\", \"LUFTTEMPERATUR\", \"REL_FEUCHTE\", \"NIEDERSCHLAGSHOEHE\", \"LUFTTEMPERATUR_MAXIMUM\", \"LUFTTEMPERATUR_MINIMUM\", \"LUFTTEMP_AM_ERDB_MINIMUM\", \"DAMPFDRUCK\", \"NIEDERSCHLAGSHOEHE_IND\", \"SCHNEEHOEHE\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Beachten des TimeLags\n",
    "Da Daten der Wetterstationen Schorndorf und Arber nur mit einem TimeLag von 3 Tagen genutzt werden dürfen müssen die jeweiligen Daten noch entsprechend verschoben werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_format = \"%d.%m.%Y\"\n",
    "\n",
    "# Incorporating Time Lag\n",
    "# Shift Arber and Schorndorf data by 3 days\n",
    "arber_data_shifted = arber_data_selected.copy()\n",
    "arber_data_shifted['DATE'] = pd.to_datetime(arber_data_shifted['DATE'], format=date_format) + pd.DateOffset(days=3)\n",
    "schorndorf_data_shifted = schorndorf_data_selected.copy()\n",
    "schorndorf_data_shifted['DATE'] = pd.to_datetime(schorndorf_data_shifted['DATE'], format=date_format) + pd.DateOffset(days=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging der Daten\n",
    "Nun können wir alle Daten zu einem Datensatz zusammenführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the Data\n",
    "# Convert 'Date' to datetime for merging\n",
    "straubing_data_selected.loc[:, 'DATE'] = pd.to_datetime(straubing_data['DATE'], format=date_format)\n",
    "\n",
    "# Add Prefix to data\n",
    "arber_data_prefixed = arber_data_shifted.add_prefix(\"arber_\")\n",
    "schorndorf_data_prefixed = schorndorf_data_shifted.add_prefix(\"schorndorf_\")\n",
    "straubing_data_prefixed = straubing_data_selected.add_prefix(\"straubing_\")\n",
    "\n",
    "merged_data = straubing_data_prefixed.merge(arber_data_prefixed, left_on='straubing_DATE', right_on=\"arber_DATE\", how='left')\n",
    "merged_data = merged_data.merge(schorndorf_data_prefixed, how=\"left\", left_on=\"straubing_DATE\", right_on=\"schorndorf_DATE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Löschen und Füllen von Daten\n",
    "Um alle NaN Daten aus unserem Datensatz zu haben gehen wir nun über die Daten und löschen NaN Daten zudem ersetzen wir -999 (fehlender Messwert) Werte mit einem Durchschnittswert der jeweiligen Zeile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data[~np.any(np.isnan(merged_data), axis=1)]\n",
    "\n",
    "column_means = merged_data.replace(-999, np.nan).mean()\n",
    "# Replace NaN values with the corresponding column mean values\n",
    "# merged_data.fillna(column_means, inplace=True) \n",
    "merged_data.replace(-999, column_means, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the data for the Neural Network\n",
    "Da wir unserem Neuronalen Netz keine \"Date\" Werte füttern können sondern lediglich numerische müssen wir uns eine neue Variable mit nur diesen Werten bilden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_data = merged_data.copy()\n",
    "used_data.drop(columns=[\"schorndorf_DATE\", \"straubing_DATE\", \"arber_DATE\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalisation\n",
    "Theoretisch haben wir die Möglichkeit nun unsere Daten zu normalisieren. Allerdings haben wir uns dafür entschieden dies nicht zu tun, da es den Realitätsbezug der Daten signifikant stört/zerstört."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization (Using Min-Max Scaling)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_data = scaler.fit_transform(used_data.select_dtypes(include=['float64', 'int64']))\n",
    "scaled_data = used_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Target Day Columns\n",
    "Um unserem Modell zu ermöglichen Vorhersagen für drei Tage im Vorraus zu machen müssen wir die relevanten Straubing Daten (entweder Temperatur oder Niederschlag) auf drei neue (verschobene) Spalten aufteilen. Am Ende müssen wir die letzten 3 Spalten dropen, da diese auf Grund der Verschiebung nun NaN Werte enthalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scaled_data to DataFrame\n",
    "scaled_data = pd.DataFrame(scaled_data, columns=column_names)\n",
    "\n",
    "scaled_data[\"Target_Day1\"] = scaled_data[\"straubing_LUFTTEMPERATUR\"].shift(-1)\n",
    "scaled_data[\"Target_Day2\"] = scaled_data[\"straubing_LUFTTEMPERATUR\"].shift(-2)\n",
    "scaled_data[\"Target_Day3\"] = scaled_data[\"straubing_LUFTTEMPERATUR\"].shift(-3)\n",
    "\n",
    "# scaled_data[\"Target_Day1\"] = scaled_data[\"straubing_NIEDERSCHLAGSHOEHE\"].shift(-1)\n",
    "# scaled_data[\"Target_Day2\"] = scaled_data[\"straubing_NIEDERSCHLAGSHOEHE\"].shift(-2)\n",
    "# scaled_data[\"Target_Day3\"] = scaled_data[\"straubing_NIEDERSCHLAGSHOEHE\"].shift(-3)\n",
    "\n",
    "scaled_data = scaled_data[:-3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting X and y\n",
    "Nun sind unsere Daten soweit vorbereitet, dass wir unser X und y selecten können. Wichtig zu beachten ist, dass X weder die Target_Day noch die originale LUFTTEMPERATUR / NIEDERSCHLAGSHOEHE enthält, da diese Werte ja die sind, die das Neuronale Netzwerk benennen soll. Genau aus diesem Grund besteht y aus diesen Werten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_data.drop(columns=[\"straubing_LUFTTEMPERATUR\", \"Target_Day1\", \"Target_Day2\", \"Target_Day3\"], axis=1)\n",
    "# X = scaled_data.drop(columns=[\"straubing_NIEDERSCHLAGSHOEHE\", \"Target_Day1\", \"Target_Day2\", \"Target_Day3\"], axis=1)\n",
    "y = scaled_data[[\"Target_Day1\", \"Target_Day2\", \"Target_Day3\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split\n",
    "Wir haben uns hier für einen 80/20 Train-Test-Split entschieden wir teilen die X und y Werte also jeweils im Verhältnis 80/20 in X_train, X_test und y_train, y_test auf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(scaled_data)*0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates Train and Test\n",
    "Um unsere Graphen im Anschluss labeln zu können brauchen wir die korrespondierenden Date Daten zu unseren Werten. Diese bekommen wir aus der vorher angelegten merged_data Variable. Um konsistent zu den anderen Daten zu sein müssen wir auch hier die letzten 3 Spalten droppen und können anschließen unsere dates_train und dates_test auf dem straubing_DATE column mit Hilfe der train_size bilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data[:-3]\n",
    "dates = merged_data[[\"straubing_DATE\"]]\n",
    "dates_train, dates_test = dates[:train_size], dates[train_size:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
