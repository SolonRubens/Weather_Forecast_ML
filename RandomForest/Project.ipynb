{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce49e81f-edc8-47ec-8fe3-524398321670",
   "metadata": {},
   "source": [
    "# Projektarbeit Weather Forecast\n",
    "1. Daten Filtern und Aufbereiten\n",
    "2. Besten Parameter für den Random Forest\n",
    "3. Ergebnis Analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec0926b-fdc8-4c25-81e6-017e9284d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import der Bibliotheken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sl\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn import neighbors\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import scipy as sc\n",
    "import math as ma\n",
    "from scipy import linalg, optimize, constants, interpolate, special, stats\n",
    "from math import exp, pow, sqrt, log\n",
    "\n",
    "import seaborn as sns #spezielle Graphikdarstellungen\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ff6d6-08d5-4536-9d2e-dae94e05a952",
   "metadata": {},
   "source": [
    "## Daten Laden\n",
    "-\n",
    "Zunächst laden wir die csv dateien in ein Pandas Dataframe auf dem die Daten im preprocessing verarbeitet werden können.\n",
    "Anschließend definieren wir eine Funktion `filter_columns`.\n",
    "Diese Funktion ermöglicht die Transformation von Daten in einem Pandas DataFrame der Daten, \n",
    "wobei zeitabhängige Verzögerungen und Varianten für bestimmte Spalten berücksichtigt werden. Die Funktion erfordert folgende Parameter:\n",
    "\n",
    "- `columns`: Eine Liste von Spalten, die für die zeitabhängigen Varianten verwendet werden sollen, einschließlich der Zielspalte.\n",
    "- `variants`: Die Anzahl der gewünschten zeitlichen Varianten.\n",
    "- `extras`: Eine Liste der Zielspalten, für die eine spezielle zeitliche Variante die abhähngig der `variants` und `timelag` in der Zukunft liegt.\n",
    "- `selected_pass`: Eine optionale Liste von Spalten, die unverändert bleiben sollen und ohne Kopien oder zeitliche Verschiebungen durch die Funktion durchgeschleust werden.\n",
    "- `timelag`: Ein optionaler Parameter, der die zeitliche Verzögerung für ausgewählte Spalten festlegt in `extras` zu den ersten Daten gibt.\n",
    "\n",
    "Die Funktion erstellt für die angegebenen Spalten zeitabhängige Varianten entsprechend der definierten Varianten und Verschiebungen. Zusätzlich werden spezielle Varianten für die in extras aufgeführten Zielspalten erstellt, wobei eine zusätzliche zeitliche Verschiebung durch timelag erfolgt. Der resultierende DataFrame wird zurückgegeben, wobei die in selected_pass aufgeführten Spalten unverändert bleiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5501c6-9bd6-473d-9fab-e72dc46ab33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['DATE', 'MESS_DATUM', 'QUALITAETS_NIVEAU', 'LUFTTEMPERATUR',\n",
      "       'DAMPFDRUCK', 'BEDECKUNGSGRAD', 'LUFTDRUCK_STATIONSHOEHE',\n",
      "       'REL_FEUCHTE', 'WINDGESCHWINDIGKEIT', 'LUFTTEMPERATUR_MAXIMUM',\n",
      "       'LUFTTEMPERATUR_MINIMUM', 'LUFTTEMP_AM_ERDB_MINIMUM',\n",
      "       'WINDSPITZE_MAXIMUM', 'NIEDERSCHLAGSHOEHE', 'NIEDERSCHLAGSHOEHE_IND',\n",
      "       'SONNENSCHEINDAUER', 'SCHNEEHOEHE'],\n",
      "      dtype='object')\n",
      "Index(['DATE', 'MESS_DATUM', 'QUALITAETS_NIVEAU', 'LUFTTEMPERATUR',\n",
      "       'DAMPFDRUCK', 'BEDECKUNGSGRAD', 'LUFTDRUCK_STATIONSHOEHE',\n",
      "       'REL_FEUCHTE', 'WINDGESCHWINDIGKEIT', 'LUFTTEMPERATUR_MAXIMUM',\n",
      "       'LUFTTEMPERATUR_MINIMUM', 'LUFTTEMP_AM_ERDB_MINIMUM',\n",
      "       'WINDSPITZE_MAXIMUM', 'NIEDERSCHLAGSHOEHE', 'NIEDERSCHLAGSHOEHE_IND',\n",
      "       'SONNENSCHEINDAUER', 'SCHNEEHOEHE'],\n",
      "      dtype='object')\n",
      "Index(['DATE', 'MESS_DATUM', 'QUALITAETS_NIVEAU', 'LUFTTEMPERATUR',\n",
      "       'DAMPFDRUCK', 'BEDECKUNGSGRAD', 'LUFTDRUCK_STATIONSHOEHE',\n",
      "       'REL_FEUCHTE', 'WINDGESCHWINDIGKEIT', 'LUFTTEMPERATUR_MAXIMUM',\n",
      "       'LUFTTEMPERATUR_MINIMUM', 'LUFTTEMP_AM_ERDB_MINIMUM',\n",
      "       'WINDSPITZE_MAXIMUM', 'NIEDERSCHLAGSHOEHE', 'NIEDERSCHLAGSHOEHE_IND',\n",
      "       'SONNENSCHEINDAUER', 'SCHNEEHOEHE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "straubingDaten = pd.read_csv(\"../data/Straubing.csv\")\n",
    "arberDaten = pd.read_csv(\"../data/Arber.csv\")\n",
    "schorndorfDaten = pd.read_csv(\"../data/Schorndorf.csv\")\n",
    "\n",
    "# Spaltennamen trimmen\n",
    "straubingDaten.columns = straubingDaten.columns.str.strip()\n",
    "arberDaten.columns = arberDaten.columns.str.strip()\n",
    "schorndorfDaten.columns = schorndorfDaten.columns.str.strip()\n",
    "\n",
    "print(straubingDaten.columns)\n",
    "print(arberDaten.columns)\n",
    "print(schorndorfDaten.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1310a897-1ae1-47bd-8a4a-3d3861aca586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(df, columns, variants=3, extras=None, selected_pass=None, timelag=0):\n",
    "    if extras is None:\n",
    "        extras = []\n",
    "    if selected_pass is None:\n",
    "        selected_pass = []\n",
    "    \n",
    "    # suffix für die spalten hinzufügen\n",
    "    df = df.rename(columns=lambda x: f'{x}_0' if x not in selected_pass else x)\n",
    "\n",
    "    # spalten kopieren und schiften um 1\n",
    "    for v in range(1, variants + 1):\n",
    "        for i in range(len(columns)):\n",
    "            df[f'{columns[i]}_{v}'] = df[f'{columns[i]}_{v-1}'].shift(-1, fill_value=-999)\n",
    "            if columns[i] in extras and v == variants:\n",
    "                df[f'{columns[i]}_{v+1}'] = df[f'{columns[i]}_{v}'].shift(-1-timelag, fill_value=-999)\n",
    "\n",
    "    # Alles wieder zusammensetzen\n",
    "    columns_with_variants = [f'{col}_{v}' for col in columns for v in range(0, variants + 1)] + [f'{col}_{variants+1}' for col in extras]\n",
    "    df_filtered = df[selected_pass + columns_with_variants]\n",
    "\n",
    "    # \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c07d420f-bd68-4f36-99b8-3832f73f6628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_values(value, df, row, col):\n",
    "    if value != -999:\n",
    "        return value\n",
    "\n",
    "    def replacement_logic(column, row, col):\n",
    "        nonlocal value\n",
    "\n",
    "        distances = np.abs(np.arange(len(column)) - row)\n",
    "\n",
    "        non_missing_values = column[column != -999]\n",
    "\n",
    "        if non_missing_values.size > 0:\n",
    "            weights = np.exp(-0.1 * distances[:len(non_missing_values)])\n",
    "            weighted_mean = np.sum(non_missing_values * weights) / np.sum(weights)\n",
    "            return weighted_mean\n",
    "\n",
    "        return -999\n",
    "\n",
    "    return replacement_logic(df.iloc[:, col].values, row, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49791c-78aa-4d1d-9ed5-0f5b41aed0d6",
   "metadata": {},
   "source": [
    "### Configuration zur Filterung\n",
    "Zunächst wird die Filterung Configuriert:\n",
    "- `selected_columns`: Eine Liste von Spalten, die für die zeitabhängigen Varianten verwendet werden sollen, einschließlich der Zielspalte.\n",
    "- `selected_result`: Eine Liste der Zielspalten.\n",
    "- `selected_pass`: Eine Liste an Spalten die keinen lediglich zum joinen von den drei datensätzen ist, diese Spalten werden später wieder rausgenommen.\n",
    "- `vergangenheit`: Der Vergangenheitswert gibt an wie weit in die vergangenheit die Daten mitgenommen wird.\n",
    "- `timelag`: Der Timelag gibt an wie weit in der zukunft die Prognose liegen muss, abhängig vom neuesten Datensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e154981-19a4-4b6b-a037-ed532535e6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['LUFTTEMPERATUR', 'DAMPFDRUCK', 'NIEDERSCHLAGSHOEHE', 'NIEDERSCHLAGSHOEHE_IND', 'SCHNEEHOEHE']\n",
    "selected_result = ['LUFTTEMPERATUR', 'NIEDERSCHLAGSHOEHE']\n",
    "selected_pass = ['MESS_DATUM']\n",
    "vergangenheit=5\n",
    "timelag = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b230ae9-9040-46e1-b37a-1e0609ee500e",
   "metadata": {},
   "source": [
    "### Filterung\n",
    "Jetzt werden die einzelnen Daten gefiltert und gejoint.\n",
    "Dabei wird für jeden Datensatz zunächst der die Verschiebungen der Zeit vorhergenommen und anschließend ein prefix hinzugefügt.\n",
    "Der Join ist ein full outer join der alle datensätze miteinander anhand der Spalte `MESS_DATUM` gejoint.\n",
    "Die Felder die übrich geblieben sind werden mit `-999` aufgefüllt.\n",
    "Die jeweiligen Spalten zum joinen werden jetzt nicht mehr gebraucht und werden gelöscht.\n",
    "Anschließend werden alle datensätze gelöscht, in denen Daten fehlen.\n",
    "Als ausgabe kann man sehen, wo und welche daten gelöscht werden.\n",
    "Zulätzt werden noch die Featcher und Ziel spalten voneinander getrennt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44b72ebe-9a9f-47f0-bbf6-3d28deff1829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Länge des Straubing Datensatzes: 23741\n",
      "Länge des Arber Datensatzes: 12114\n",
      "Länge des Schorndorf Datensatzes: 7305\n",
      "#################\n",
      "Anzahl wie viele daten in einer Spalte Fehlen:\n",
      "straubing_LUFTTEMPERATUR_0             0\n",
      "straubing_LUFTTEMPERATUR_1             0\n",
      "straubing_LUFTTEMPERATUR_2             0\n",
      "straubing_LUFTTEMPERATUR_3             0\n",
      "straubing_LUFTTEMPERATUR_4             0\n",
      "straubing_LUFTTEMPERATUR_5             0\n",
      "straubing_DAMPFDRUCK_0                 0\n",
      "straubing_DAMPFDRUCK_1                 0\n",
      "straubing_DAMPFDRUCK_2                 0\n",
      "straubing_DAMPFDRUCK_3                 0\n",
      "straubing_DAMPFDRUCK_4                 0\n",
      "straubing_DAMPFDRUCK_5                 0\n",
      "straubing_NIEDERSCHLAGSHOEHE_0         0\n",
      "straubing_NIEDERSCHLAGSHOEHE_1         0\n",
      "straubing_NIEDERSCHLAGSHOEHE_2         0\n",
      "straubing_NIEDERSCHLAGSHOEHE_3         0\n",
      "straubing_NIEDERSCHLAGSHOEHE_4         0\n",
      "straubing_NIEDERSCHLAGSHOEHE_5         0\n",
      "straubing_NIEDERSCHLAGSHOEHE_IND_0     0\n",
      "straubing_NIEDERSCHLAGSHOEHE_IND_1     0\n",
      "straubing_NIEDERSCHLAGSHOEHE_IND_2     0\n",
      "straubing_NIEDERSCHLAGSHOEHE_IND_3     0\n",
      "straubing_NIEDERSCHLAGSHOEHE_IND_4     0\n",
      "straubing_NIEDERSCHLAGSHOEHE_IND_5     0\n",
      "straubing_SCHNEEHOEHE_0                0\n",
      "straubing_SCHNEEHOEHE_1                0\n",
      "straubing_SCHNEEHOEHE_2                0\n",
      "straubing_SCHNEEHOEHE_3                0\n",
      "straubing_SCHNEEHOEHE_4                0\n",
      "straubing_SCHNEEHOEHE_5                0\n",
      "straubing_LUFTTEMPERATUR_6             0\n",
      "straubing_NIEDERSCHLAGSHOEHE_6         0\n",
      "arber_LUFTTEMPERATUR_0                 0\n",
      "arber_LUFTTEMPERATUR_1                 0\n",
      "arber_LUFTTEMPERATUR_2                 0\n",
      "arber_LUFTTEMPERATUR_3                 0\n",
      "arber_LUFTTEMPERATUR_4                 0\n",
      "arber_LUFTTEMPERATUR_5                 0\n",
      "arber_DAMPFDRUCK_0                     0\n",
      "arber_DAMPFDRUCK_1                     0\n",
      "arber_DAMPFDRUCK_2                     0\n",
      "arber_DAMPFDRUCK_3                     0\n",
      "arber_DAMPFDRUCK_4                     0\n",
      "arber_DAMPFDRUCK_5                     0\n",
      "arber_NIEDERSCHLAGSHOEHE_0             0\n",
      "arber_NIEDERSCHLAGSHOEHE_1             0\n",
      "arber_NIEDERSCHLAGSHOEHE_2             0\n",
      "arber_NIEDERSCHLAGSHOEHE_3             0\n",
      "arber_NIEDERSCHLAGSHOEHE_4             0\n",
      "arber_NIEDERSCHLAGSHOEHE_5             0\n",
      "arber_NIEDERSCHLAGSHOEHE_IND_0         0\n",
      "arber_NIEDERSCHLAGSHOEHE_IND_1         0\n",
      "arber_NIEDERSCHLAGSHOEHE_IND_2         0\n",
      "arber_NIEDERSCHLAGSHOEHE_IND_3         0\n",
      "arber_NIEDERSCHLAGSHOEHE_IND_4         0\n",
      "arber_NIEDERSCHLAGSHOEHE_IND_5         0\n",
      "arber_SCHNEEHOEHE_0                    0\n",
      "arber_SCHNEEHOEHE_1                    0\n",
      "arber_SCHNEEHOEHE_2                    0\n",
      "arber_SCHNEEHOEHE_3                    0\n",
      "arber_SCHNEEHOEHE_4                    0\n",
      "arber_SCHNEEHOEHE_5                    0\n",
      "schorndorf_LUFTTEMPERATUR_0            0\n",
      "schorndorf_LUFTTEMPERATUR_1            0\n",
      "schorndorf_LUFTTEMPERATUR_2            0\n",
      "schorndorf_LUFTTEMPERATUR_3            0\n",
      "schorndorf_LUFTTEMPERATUR_4            0\n",
      "schorndorf_LUFTTEMPERATUR_5            0\n",
      "schorndorf_DAMPFDRUCK_0                0\n",
      "schorndorf_DAMPFDRUCK_1                0\n",
      "schorndorf_DAMPFDRUCK_2                0\n",
      "schorndorf_DAMPFDRUCK_3                0\n",
      "schorndorf_DAMPFDRUCK_4                0\n",
      "schorndorf_DAMPFDRUCK_5                0\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_0        0\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_1        0\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_2        0\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_3        0\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_4        0\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_5        0\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_IND_0    0\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_IND_1    0\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_IND_2    0\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_IND_3    0\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_IND_4    0\n",
      "schorndorf_NIEDERSCHLAGSHOEHE_IND_5    0\n",
      "schorndorf_SCHNEEHOEHE_0               0\n",
      "schorndorf_SCHNEEHOEHE_1               0\n",
      "schorndorf_SCHNEEHOEHE_2               0\n",
      "schorndorf_SCHNEEHOEHE_3               0\n",
      "schorndorf_SCHNEEHOEHE_4               0\n",
      "schorndorf_SCHNEEHOEHE_5               0\n",
      "dtype: int64\n",
      "#################\n",
      "Gesamte Datengröße: 23862\n",
      "Verlorene Daten durch die Filterung: 0\n",
      "Finale Datengröße: 23862\n"
     ]
    }
   ],
   "source": [
    "# Zeitverschiebungen hinzugefügt\n",
    "straubingDatenFiltered = filter_columns(straubingDaten, selected_columns, variants=vergangenheit, extras=selected_result, selected_pass=selected_pass, timelag=timelag)\n",
    "arberDatenFiltered = filter_columns(arberDaten, selected_columns, variants=vergangenheit, extras=[], selected_pass=selected_pass, timelag=timelag)\n",
    "schorndorfDatenFiltered = filter_columns(schorndorfDaten, selected_columns, variants=vergangenheit, extras=[], selected_pass=selected_pass, timelag=timelag)\n",
    "\n",
    "# Prefixes hinzugefügt\n",
    "straubingDatenFiltered = straubingDatenFiltered.add_prefix('straubing_')\n",
    "arberDatenFiltered = arberDatenFiltered.add_prefix('arber_')\n",
    "schorndorfDatenFiltered = schorndorfDatenFiltered.add_prefix('schorndorf_')\n",
    "\n",
    "# Ausgabe der Länge der einzelnen Datensätze\n",
    "print(f'Länge des Straubing Datensatzes: {len(straubingDatenFiltered)}')\n",
    "print(f'Länge des Arber Datensatzes: {len(arberDatenFiltered)}')\n",
    "print(f'Länge des Schorndorf Datensatzes: {len(schorndorfDatenFiltered)}')\n",
    "print('#################')\n",
    "\n",
    "# Merging der Einzelnen Datensätze zu einem neuen (allDatenFiltered)\n",
    "allDatenFiltered = pd.merge(straubingDatenFiltered, arberDatenFiltered, how='outer', left_on='straubing_MESS_DATUM', right_on='arber_MESS_DATUM').merge(schorndorfDatenFiltered, how='outer', left_on='straubing_MESS_DATUM', right_on='schorndorf_MESS_DATUM')\n",
    "allDatenFiltered = allDatenFiltered.fillna(-999)\n",
    "allDatenFiltered = allDatenFiltered.drop(columns=['straubing_MESS_DATUM', 'arber_MESS_DATUM', 'schorndorf_MESS_DATUM'])\n",
    "\n",
    "\n",
    "# Iteriere über Zeilen und Spalten\n",
    "for row in range(allDatenFiltered.shape[0]):\n",
    "    for col in range(allDatenFiltered.shape[1]):\n",
    "        allDatenFiltered.iloc[row, col] = replace_missing_values(allDatenFiltered.iloc[row, col], allDatenFiltered, row, col)\n",
    "\n",
    "\n",
    "# Ausgabe der zu löschenden Daten und diese dann auch gelöscht\n",
    "# Hier wird noch ein unterschied für die prefixes gemacht, dies ist nur noch aus lagacy gründen drinnen\n",
    "preflen = len(allDatenFiltered)\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(f'Anzahl wie viele daten in einer Spalte Fehlen:\\n{(allDatenFiltered == -999).sum()}')\n",
    "pd.reset_option('display.max_rows')\n",
    "for prefix in ['straubing_', 'arber_', 'schorndorf_']:\n",
    "    mask = allDatenFiltered.columns.str.startswith(prefix)\n",
    "    allDatenFiltered = allDatenFiltered[~allDatenFiltered.loc[:, mask].isin([-999]).any(axis=1)]\n",
    "allDatenFiltered = allDatenFiltered.sample(frac=1).reset_index(drop=True)\n",
    "print('#################')\n",
    "print(f'Gesamte Datengröße: {preflen}\\nVerlorene Daten durch die Filterung: {preflen - len(allDatenFiltered)}\\nFinale Datengröße: {len(allDatenFiltered)}')\n",
    "\n",
    "# Seperation von Featchers und Zieldaten (x, y)\n",
    "Y_Daten_Temp = allDatenFiltered.loc[:, [f'straubing_LUFTTEMPERATUR_{vergangenheit+1}']].copy()\n",
    "X_Daten_Temp = allDatenFiltered.drop(columns=[f'straubing_LUFTTEMPERATUR_{vergangenheit+1}']).drop(columns=[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit+1}'])\n",
    "\n",
    "Y_Daten_Reg = allDatenFiltered.loc[:, [f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit+1}']].copy()\n",
    "X_Daten_Reg = allDatenFiltered.drop(columns=[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit+1}']).drop(columns=[f'straubing_LUFTTEMPERATUR_{vergangenheit+1}'])\n",
    "\n",
    "X_Temp = X_Daten_Temp.values\n",
    "Y_Temp = Y_Daten_Temp.values.flatten()\n",
    "\n",
    "X_Reg = X_Daten_Reg.values\n",
    "Y_Reg = Y_Daten_Reg.values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4662e981-bf57-480a-92ad-a9573e1e69a7",
   "metadata": {},
   "source": [
    "## Parameter für den Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74ae0c23-03d9-4b9b-b6ec-8ebf6ee499d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der benötigten Bibliotheken\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "514d8430-217d-4033-adf8-e592a5f5c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, n_estimators, max_depth, test_size):\n",
    "    # Train Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size)\n",
    "\n",
    "    # Initialisieren des Random Forest Regressors\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=n_estimators, n_jobs=-1, max_depth=max_depth)\n",
    "    \n",
    "    # Trainieren des Regressors mit den Trainingsdaten\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Vorhersagen auf den Testdaten\n",
    "    y_pred_test = rf_regressor.predict(X_test)\n",
    "    y_pred_train = rf_regressor.predict(X_train)\n",
    "\n",
    "    return (mean_squared_error(y_test, y_pred_test), mean_squared_error(y_train, y_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bd41f4-5bd6-4f39-b0ae-b87e694a1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_range = range(5, 100, 5)\n",
    "depth_range = range(5, 100, 5)\n",
    "split_range = range(5, 90, 5)\n",
    "\n",
    "estimators_mse_test_Temp = []\n",
    "estimators_mse_train_Temp = []\n",
    "depth_mse_test_Temp = []\n",
    "depth_mse_train_Temp = []\n",
    "split_mse_test_Temp = []\n",
    "split_mse_train_Temp = []\n",
    "estimators_mse_test_Reg = []\n",
    "estimators_mse_train_Reg = []\n",
    "depth_mse_test_Reg = []\n",
    "depth_mse_train_Reg = []\n",
    "split_mse_test_Reg = []\n",
    "split_mse_train_Reg = []\n",
    "\n",
    "for est in estimators_range:\n",
    "    mse_test, mse_train = train(X_Temp, Y_Temp, est, 20, 0.2)\n",
    "    estimators_mse_test_Temp.append(mse_test)\n",
    "    estimators_mse_train_Temp.append(mse_train)\n",
    "    \n",
    "for depth in depth_range:\n",
    "    mse_test, mse_train = train(X_Temp, Y_Temp, 50, depth, 0.2)\n",
    "    depth_mse_test_Temp.append(mse_test)\n",
    "    depth_mse_train_Temp.append(mse_train)\n",
    "    \n",
    "for split in split_range:\n",
    "    mse_test, mse_train = train(X_Temp, Y_Temp, 50, 20, split/100)\n",
    "    split_mse_test_Temp.append(mse_test)\n",
    "    split_mse_train_Temp.append(mse_train)\n",
    "\n",
    "for est in estimators_range:\n",
    "    mse_test, mse_train = train(X_Reg, Y_Reg, est, 40, 0.2)\n",
    "    estimators_mse_test_Reg.append(mse_test)\n",
    "    estimators_mse_train_Reg.append(mse_train)\n",
    "    \n",
    "for depth in depth_range:\n",
    "    mse_test, mse_train = train(X_Reg, Y_Reg, 50, depth, 0.2)\n",
    "    depth_mse_test_Reg.append(mse_test)\n",
    "    depth_mse_train_Reg.append(mse_train)\n",
    "    \n",
    "for split in split_range:\n",
    "    mse_test, mse_train = train(X_Reg, Y_Reg, 50, 40, split/100)\n",
    "    split_mse_test_Reg.append(mse_test)\n",
    "    split_mse_train_Reg.append(mse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a676ec35-3bcb-4d47-8dd4-052d41809b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen von Subplots (2 Zeilen, 3 Spalten)\n",
    "fig, axs = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Forest Trees\n",
    "axs[0, 0].plot(estimators_range, estimators_mse_test_Temp, marker='o', label='Test MSE')\n",
    "axs[0, 0].plot(estimators_range, estimators_mse_train_Temp, marker='o', label='Train MSE')\n",
    "axs[0, 0].set_ylabel('MSE (Mean Squared Error)')\n",
    "axs[0, 0].set_xlabel('Trees')\n",
    "axs[0, 0].set_title('Forest Trees')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Plot 2: Tree Depth\n",
    "axs[0, 1].plot(depth_range, depth_mse_test_Temp, marker='o', label='Test MSE')\n",
    "axs[0, 1].plot(depth_range, depth_mse_train_Temp, marker='o', label='Train MSE')\n",
    "axs[0, 1].set_ylabel('MSE (Mean Squared Error)')\n",
    "axs[0, 1].set_xlabel('Depth')\n",
    "axs[0, 1].set_title('Tree Depth')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Plot 3: Train Test Split\n",
    "axs[0, 2].plot([split/100 for split in split_range], split_mse_test_Temp, marker='o', label='Test MSE')\n",
    "axs[0, 2].plot([split/100 for split in split_range], split_mse_train_Temp, marker='o', label='Train MSE')\n",
    "axs[0, 2].set_ylabel('MSE (Mean Squared Error)')\n",
    "axs[0, 2].set_xlabel('Split')\n",
    "axs[0, 2].set_title('Train Test Split')\n",
    "axs[0, 2].legend()\n",
    "\n",
    "# Plot 4: Forest Trees\n",
    "axs[1, 0].plot(estimators_range, estimators_mse_test_Reg, marker='o', label='Test MSE')\n",
    "axs[1, 0].plot(estimators_range, estimators_mse_train_Reg, marker='o', label='Train MSE')\n",
    "axs[1, 0].set_ylabel('MSE (Mean Squared Error)')\n",
    "axs[1, 0].set_xlabel('Trees')\n",
    "axs[1, 0].set_title('Forest Trees')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Plot 5: Tree Depth\n",
    "axs[1, 1].plot(depth_range, depth_mse_test_Reg, marker='o', label='Test MSE')\n",
    "axs[1, 1].plot(depth_range, depth_mse_train_Reg, marker='o', label='Train MSE')\n",
    "axs[1, 1].set_ylabel('MSE (Mean Squared Error)')\n",
    "axs[1, 1].set_xlabel('Depth')\n",
    "axs[1, 1].set_title('Tree Depth')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# Plot 6: Train Test Split\n",
    "axs[1, 2].plot([split/100 for split in split_range], split_mse_test_Reg, marker='o', label='Test MSE')\n",
    "axs[1, 2].plot([split/100 for split in split_range], split_mse_train_Reg, marker='o', label='Train MSE')\n",
    "axs[1, 2].set_ylabel('MSE (Mean Squared Error)')\n",
    "axs[1, 2].set_xlabel('Split')\n",
    "axs[1, 2].set_title('Train Test Split')\n",
    "axs[1, 2].legend()\n",
    "\n",
    "# Layout anpassen\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e72920-1b25-46a7-b7b8-61160580fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Train Test split\n",
    "X_train_Temp, X_test_Temp, y_train_Temp, y_test_Temp = train_test_split(X_Temp, Y_Temp, test_size=0.2)\n",
    "X_train_Reg, X_test_Reg, y_train_Reg, y_test_Reg = train_test_split(X_Reg, Y_Reg, test_size=0.2)\n",
    "\n",
    "# Gradient Boosting für die Temperatur-Vorhersagen\n",
    "gb_regressor_Temp = GradientBoostingRegressor(n_estimators=100,  # Anzahl der Bäume im Ensemble\n",
    "                                              learning_rate=0.1,  # Lernrate\n",
    "                                              max_depth=4,  # Maximale Tiefe der Bäume\n",
    "                                              random_state=42)\n",
    "\n",
    "# Trainieren des Gradient Boosting-Modells mit den Trainingsdaten\n",
    "gb_regressor_Temp.fit(X_train_Temp, y_train_Temp)\n",
    "\n",
    "# Vorhersagen auf den Testdaten\n",
    "y_pred_test_Temp = gb_regressor_Temp.predict(X_test_Temp)\n",
    "y_pred_train_Temp = gb_regressor_Temp.predict(X_train_Temp)\n",
    "\n",
    "# Auswertung des Gradient Boosting-Modells für die Temperatur\n",
    "print(\"RMSE Temperatur Vortag: \" + str(mean_squared_error(Y_Daten_Temp, X_Daten_Temp[f'straubing_LUFTTEMPERATUR_{vergangenheit}'])))\n",
    "print(\"RMSE Temperatur Test:   \" + str(mean_squared_error(y_test_Temp, y_pred_test_Temp)))\n",
    "print(\"RMSE Temperatur Train:  \" + str(mean_squared_error(y_train_Temp, y_pred_train_Temp)))\n",
    "\n",
    "# Gradient Boosting für die Niederschlags-Vorhersagen\n",
    "gb_regressor_Reg = GradientBoostingRegressor(n_estimators=100,  # Anzahl der Bäume im Ensemble\n",
    "                                             learning_rate=0.1,  # Lernrate\n",
    "                                             max_depth=4,  # Maximale Tiefe der Bäume\n",
    "                                             random_state=42)\n",
    "\n",
    "# Trainieren des Gradient Boosting-Modells mit den Trainingsdaten\n",
    "gb_regressor_Reg.fit(X_train_Reg, y_train_Reg)\n",
    "\n",
    "# Vorhersagen auf den Testdaten\n",
    "y_pred_test_Reg = gb_regressor_Reg.predict(X_test_Reg)\n",
    "y_pred_train_Reg = gb_regressor_Reg.predict(X_train_Reg)\n",
    "\n",
    "# Auswertung des Gradient Boosting-Modells für den Niederschlag\n",
    "print(\"RMSE Niederschlag Vortag: \" + str(mean_squared_error(Y_Daten_Reg, X_Daten_Reg[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit}'])))\n",
    "print(\"RMSE Niederschlag Test:   \" + str(mean_squared_error(y_test_Reg, y_pred_test_Reg)))\n",
    "print(\"RMSE Niederschlag Train:  \" + str(mean_squared_error(y_train_Reg, y_pred_train_Reg)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2dff5-490d-45d8-b6b5-e36394de8fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# Train Test split\n",
    "X_train_Temp, X_test_Temp, y_train_Temp, y_test_Temp = train_test_split(X_Temp, Y_Temp, test_size=0.2)\n",
    "X_train_Reg, X_test_Reg, y_train_Reg, y_test_Reg = train_test_split(X_Reg, Y_Reg, test_size=0.2)\n",
    "\n",
    "# Bagging für die Temperatur-Vorhersagen\n",
    "bagging_regressor_Temp = BaggingRegressor(estimator=RandomForestRegressor(n_estimators=50, n_jobs=-1, max_depth=20),\n",
    "                                          n_estimators=10,  # Anzahl der Modelle im Ensemble\n",
    "                                          random_state=42)\n",
    "\n",
    "# Trainieren des Bagging-Modells mit den Trainingsdaten\n",
    "bagging_regressor_Temp.fit(X_train_Temp, y_train_Temp)\n",
    "\n",
    "# Vorhersagen auf den Testdaten\n",
    "y_pred_test_Temp = bagging_regressor_Temp.predict(X_test_Temp)\n",
    "y_pred_train_Temp = bagging_regressor_Temp.predict(X_train_Temp)\n",
    "\n",
    "# Auswertung des Bagging-Modells für die Temperatur\n",
    "print(\"RMSE Temperatur Vortag: \" + str(mean_squared_error(Y_Daten_Temp, X_Daten_Temp[f'straubing_LUFTTEMPERATUR_{vergangenheit}'])))\n",
    "print(\"RMSE Temperatur Test:   \" + str(mean_squared_error(y_test_Temp, y_pred_test_Temp)))\n",
    "print(\"RMSE Temperatur Train:  \" + str(mean_squared_error(y_train_Temp, y_pred_train_Temp)))\n",
    "\n",
    "# Bagging für die Niederschlags-Vorhersagen\n",
    "bagging_regressor_Reg = BaggingRegressor(estimator=RandomForestRegressor(n_estimators=50, n_jobs=-1, max_depth=40),\n",
    "                                         n_estimators=10,  # Anzahl der Modelle im Ensemble\n",
    "                                         random_state=42)\n",
    "\n",
    "# Trainieren des Bagging-Modells mit den Trainingsdaten\n",
    "bagging_regressor_Reg.fit(X_train_Reg, y_train_Reg)\n",
    "\n",
    "# Vorhersagen auf den Testdaten\n",
    "y_pred_test_Reg = bagging_regressor_Reg.predict(X_test_Reg)\n",
    "y_pred_train_Reg = bagging_regressor_Reg.predict(X_train_Reg)\n",
    "\n",
    "# Auswertung des Bagging-Modells für den Niederschlag\n",
    "print(\"RMSE Niederschlag Vortag: \" + str(mean_squared_error(Y_Daten_Reg, X_Daten_Reg[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit}'])))\n",
    "print(\"RMSE Niederschlag Test:   \" + str(mean_squared_error(y_test_Reg, y_pred_test_Reg)))\n",
    "print(\"RMSE Niederschlag Train:  \" + str(mean_squared_error(y_train_Reg, y_pred_train_Reg)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c506f631-044f-4866-a25a-74a844da1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Test split\n",
    "X_train_Temp, X_test_Temp, y_train_Temp, y_test_Temp = train_test_split(X_Temp, Y_Temp, test_size=0.2)\n",
    "X_train_Reg, X_test_Reg, y_train_Reg, y_test_Reg = train_test_split(X_Reg, Y_Reg, test_size=0.2)\n",
    "\n",
    "# Initialisieren des Random Forest Regressors\n",
    "rf_regressor_Temp = RandomForestRegressor(n_estimators=50, n_jobs=-1, max_depth=20)\n",
    "rf_regressor_Reg = RandomForestRegressor(n_estimators=50, n_jobs=-1, max_depth=40)\n",
    "    \n",
    "# Trainieren des Regressors mit den Trainingsdaten\n",
    "rf_regressor_Temp.fit(X_train_Temp, y_train_Temp)\n",
    "rf_regressor_Reg.fit(X_train_Reg, y_train_Reg)\n",
    "    \n",
    "# Vorhersagen auf den Testdaten\n",
    "y_pred_test_Temp = rf_regressor_Temp.predict(X_test_Temp)\n",
    "y_pred_train_Temp = rf_regressor_Temp.predict(X_train_Temp)\n",
    "y_pred_test_Reg = rf_regressor_Reg.predict(X_test_Reg)\n",
    "y_pred_train_Reg = rf_regressor_Reg.predict(X_train_Reg)\n",
    "    \n",
    "# Auswertung des Modells (z.B. Mean Squared Error für Regression)\n",
    "print(\"RMSE Temperatur Vortag:\" + str(mean_squared_error(Y_Daten_Temp, X_Daten_Temp[f'straubing_LUFTTEMPERATUR_{vergangenheit}'])))\n",
    "print(\"RMSE Temperatur Test:  \" + str(mean_squared_error(y_test_Temp, y_pred_test_Temp)))\n",
    "print(\"RMSE Temperatur Train: \" + str(mean_squared_error(y_train_Temp, y_pred_train_Temp)))\n",
    "print(\"RMSE Niderschlag Vortag:\" + str(mean_squared_error(Y_Daten_Reg, X_Daten_Reg[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit}'])))\n",
    "print(\"RMSE Niderschlag Test:  \" + str(mean_squared_error(y_test_Reg, y_pred_test_Reg)))\n",
    "print(\"RMSE Niderschlag Train: \" + str(mean_squared_error(y_train_Reg, y_pred_train_Reg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5138eef8-70d5-494f-a8f1-8773a0685ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplots nebeneinander darstellen\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Scatterplot im Traindatensatz\n",
    "axs[0, 0].scatter(Y_Daten_Temp, X_Daten_Temp[f'straubing_LUFTTEMPERATUR_{vergangenheit}'], alpha=0.5, label='Verhältnist zur vortags Temperatur')\n",
    "axs[0, 0].scatter(y_train_Temp, y_pred_train_Temp, alpha=0.5, label='Vorhersagen')\n",
    "axs[0, 0].plot(axs[0, 0].get_xlim(), axs[0, 0].get_xlim(), color='red', linestyle='--', label='Ideallinie')\n",
    "axs[0, 0].set_xlabel('Tatsächliche Werte')\n",
    "axs[0, 0].set_ylabel('Vorhersagen')\n",
    "axs[0, 0].set_title('Vergleich der tatsächlichen Werte und Vorhersagen im Traindatensatz')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Scatterplot im Testdatensatz\n",
    "axs[0, 1].scatter(Y_Daten_Temp, X_Daten_Temp[f'straubing_LUFTTEMPERATUR_{vergangenheit}'], alpha=0.5, label='Verhältnist zur vortags Temperatur')\n",
    "axs[0, 1].scatter(y_test_Temp, y_pred_test_Temp, alpha=0.5, label='Vorhersagen')\n",
    "axs[0, 1].plot(axs[0, 1].get_xlim(), axs[0, 1].get_xlim(), color='red', linestyle='--', label='Ideallinie')\n",
    "axs[0, 1].set_xlabel('Tatsächliche Werte')\n",
    "axs[0, 1].set_ylabel('Vorhersagen')\n",
    "axs[0, 1].set_title('Vergleich der tatsächlichen Werte und Vorhersagen im Testdatensatz')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Scatterplot im Traindatensatz\n",
    "axs[1, 0].scatter(Y_Daten_Reg, X_Daten_Reg[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit}'], alpha=0.5, label='Verhältnist zur vortags Temperatur')\n",
    "axs[1, 0].scatter(y_train_Reg, y_pred_train_Reg, alpha=0.5, label='Vorhersagen')\n",
    "axs[1, 0].plot(axs[1, 0].get_xlim(), axs[1, 0].get_xlim(), color='red', linestyle='--', label='Ideallinie')\n",
    "axs[1, 0].set_xlabel('Tatsächliche Werte')\n",
    "axs[1, 0].set_ylabel('Vorhersagen')\n",
    "axs[1, 0].set_title('Vergleich der tatsächlichen Werte und Vorhersagen im Traindatensatz')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Scatterplot im Testdatensatz\n",
    "axs[1, 1].scatter(Y_Daten_Reg, X_Daten_Reg[f'straubing_NIEDERSCHLAGSHOEHE_{vergangenheit}'], alpha=0.5, label='Verhältnist zur vortags Temperatur')\n",
    "axs[1, 1].scatter(y_test_Reg, y_pred_test_Reg, alpha=0.5, label='Vorhersagen')\n",
    "axs[1, 1].plot(axs[1, 1].get_xlim(), axs[1, 1].get_xlim(), color='red', linestyle='--', label='Ideallinie')\n",
    "axs[1, 1].set_xlabel('Tatsächliche Werte')\n",
    "axs[1, 1].set_ylabel('Vorhersagen')\n",
    "axs[1, 1].set_title('Vergleich der tatsächlichen Werte und Vorhersagen im Testdatensatz')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0529f3e6-cb59-470b-80a0-c360830d32e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unterschied der Arrays berechnen\n",
    "runden = 1\n",
    "diff_array_test_Temp = np.abs(np.round(y_test_Temp * runden) / runden - np.round(y_pred_test_Temp * runden) / runden)\n",
    "diff_array_train_Temp = np.abs(np.round(y_train_Temp * runden) / runden - np.round(y_pred_train_Temp * runden) / runden)\n",
    "diff_array_test_Reg = np.abs(np.round(y_test_Reg * runden) / runden - np.round(y_pred_test_Reg * runden) / runden)\n",
    "diff_array_train_Reg = np.abs(np.round(y_train_Reg * runden) / runden - np.round(y_pred_train_Reg * runden) / runden)\n",
    "\n",
    "unique_elements_test_Temp, counts_test_Temp = np.unique(diff_array_test_Temp, return_counts=True)\n",
    "unique_elements_train_Temp, counts_train_Temp = np.unique(diff_array_train_Temp, return_counts=True)\n",
    "unique_elements_test_Reg, counts_test_Reg = np.unique(diff_array_test_Reg, return_counts=True)\n",
    "unique_elements_train_Reg, counts_train_Reg = np.unique(diff_array_train_Reg, return_counts=True)\n",
    "\n",
    "# Werte-Bereich mit dem gewünschten Abstand erstellen\n",
    "max_value_Temp = max(max(unique_elements_test_Temp), max(unique_elements_train_Temp))\n",
    "max_value_Reg = max(max(unique_elements_test_Reg), max(unique_elements_train_Reg))\n",
    "def expand(min, max, step, val, count):\n",
    "    all_values = np.arange(min, max + step, step)\n",
    "    # Index-Array für vorhandene Werte erstellen\n",
    "    existing_values_idx = np.isin(all_values, val)\n",
    "    # Fehlende Werte mit Häufigkeit 0 einfügen\n",
    "    all_frequencies = np.zeros_like(all_values, dtype=int)\n",
    "    all_frequencies[existing_values_idx] = count\n",
    "    return (all_values, all_frequencies)\n",
    "\n",
    "unique_elements_test_Temp, counts_test_Temp = expand(0, max_value_Temp, 1/runden, unique_elements_test_Temp, counts_test_Temp)\n",
    "unique_elements_train_Temp, counts_train_Temp = expand(0, max_value_Temp, 1/runden, unique_elements_train_Temp, counts_train_Temp)\n",
    "unique_elements_test_Reg, counts_test_Reg = expand(0, max_value_Reg, 1/runden, unique_elements_test_Reg, counts_test_Reg)\n",
    "unique_elements_train_Reg, counts_train_Reg = expand(0, max_value_Reg, 1/runden, unique_elements_train_Reg, counts_train_Reg)\n",
    "\n",
    "\n",
    "# Verteilung plotten als Bar Plot\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(20, 14))\n",
    "\n",
    "# Breite der Bars\n",
    "bar_width = 0.5\n",
    "\n",
    "# Positionen der x-Ticks für die beiden Gruppen\n",
    "x_ticks_test_Temp = np.arange(len(unique_elements_test_Temp))\n",
    "x_ticks_train_Temp = np.arange(len(unique_elements_train_Temp))\n",
    "x_ticks_test_Reg = np.arange(len(unique_elements_test_Reg))\n",
    "x_ticks_train_Reg = np.arange(len(unique_elements_train_Reg))\n",
    "\n",
    "# Plot für Testdaten\n",
    "bar_test_Temp = ax1.bar(x_ticks_test_Temp, counts_test_Temp, width=bar_width, label=\"Test Daten\")\n",
    "ax1.set_xlabel('Grad')\n",
    "ax1.set_ylabel('Häufigkeit')\n",
    "ax1.set_xticks(x_ticks_test_Temp)\n",
    "ax1.set_xticklabels(unique_elements_test_Temp)\n",
    "ax1.legend()\n",
    "ax1.set_title('Verteilung des absoluten Fehlers der Testdaten')\n",
    "\n",
    "# Text über den Bars anzeigen\n",
    "for bar, counts in zip(bar_test_Temp, counts_test_Temp):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width() / 2, height, f'{counts}', \n",
    "            ha='center', va='bottom')\n",
    "\n",
    "# Plot für Trainingsdaten\n",
    "bar_train_Temp = ax2.bar(x_ticks_train_Temp, counts_train_Temp, width=bar_width, label=\"Trainings Daten\")\n",
    "ax2.set_xlabel('Grad')\n",
    "ax2.set_ylabel('Häufigkeit')\n",
    "ax2.set_xticks(x_ticks_train_Temp)\n",
    "ax2.set_xticklabels(unique_elements_train_Temp)\n",
    "ax2.legend()\n",
    "ax2.set_title('Verteilung des absoluten Fehlers der Trainingsdaten')\n",
    "\n",
    "# Text über den Bars anzeigen\n",
    "for bar, counts in zip(bar_train_Temp, counts_train_Temp):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2, height, f'{counts}', \n",
    "            ha='center', va='bottom')\n",
    "\n",
    "\n",
    "# Plot für Testdaten\n",
    "bar_test_Reg = ax3.bar(x_ticks_test_Reg, counts_test_Reg, width=bar_width, label=\"Test Daten\")\n",
    "ax3.set_xlabel('Grad')\n",
    "ax3.set_ylabel('Häufigkeit')\n",
    "ax3.set_xticks(x_ticks_test_Reg)\n",
    "ax3.set_xticklabels(unique_elements_test_Reg)\n",
    "ax3.legend()\n",
    "ax3.set_title('Verteilung des absoluten Fehlers der Testdaten')\n",
    "\n",
    "# Text über den Bars anzeigen\n",
    "for bar, counts in zip(bar_test_Reg, counts_test_Reg):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width() / 2, height, f'{counts}', \n",
    "            ha='center', va='bottom')\n",
    "\n",
    "# Plot für Trainingsdaten\n",
    "bar_train_Reg = ax4.bar(x_ticks_train_Reg, counts_train_Reg, width=bar_width, label=\"Trainings Daten\")\n",
    "ax4.set_xlabel('Grad')\n",
    "ax4.set_ylabel('Häufigkeit')\n",
    "ax4.set_xticks(x_ticks_train_Reg)\n",
    "ax4.set_xticklabels(unique_elements_train_Reg)\n",
    "ax4.legend()\n",
    "ax4.set_title('Verteilung des absoluten Fehlers der Trainingsdaten')\n",
    "\n",
    "# Text über den Bars anzeigen\n",
    "for bar, counts in zip(bar_train_Reg, counts_train_Reg):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width() / 2, height, f'{counts}', \n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb3e891-aa79-4fbe-9cd5-598a9d7102e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrahieren der Importance der Trees\n",
    "importances_Temp = np.round(rf_regressor_Temp.feature_importances_*100, 2)\n",
    "std_Temp = np.round(np.std([tree.feature_importances_ for tree in rf_regressor_Temp.estimators_], axis=0)*100, 2)\n",
    "importances_Reg = np.round(rf_regressor_Reg.feature_importances_*100, 2)\n",
    "std_Reg = np.round(np.std([tree.feature_importances_ for tree in rf_regressor_Reg.estimators_], axis=0)*100, 2)\n",
    "\n",
    "# Indizes der sortierten Importance-Werte\n",
    "sorted_indices_Temp = np.argsort(importances_Temp)\n",
    "sorted_indices_Reg = np.argsort(importances_Reg)\n",
    "\n",
    "# Sortieren der Importance und Standardabweichung\n",
    "importances_Temp = importances_Temp[sorted_indices_Temp]\n",
    "std_Temp = std_Temp[sorted_indices_Temp]\n",
    "importances_Reg = importances_Reg[sorted_indices_Reg]\n",
    "std_Reg = std_Reg[sorted_indices_Reg]\n",
    "\n",
    "forest_importances_Temp = pd.Series(importances_Temp, index=X_Daten_Temp.columns[sorted_indices_Temp])\n",
    "forest_importances_Reg = pd.Series(importances_Reg, index=X_Daten_Reg.columns[sorted_indices_Reg])\n",
    "\n",
    "# Breite des Bildes anpassen\n",
    "fig, ax = plt.subplots(figsize=(8, 30))\n",
    "\n",
    "# Balkenplot mit Fehlerbalken\n",
    "forest_importances_Temp.plot.barh(xerr=std_Temp, ax=ax, capsize=4)\n",
    "\n",
    "# Titel und Achsenbeschriftungen\n",
    "ax.set_title(\"Feature importances with uncertainty\")\n",
    "ax.set_xlabel(\"Mean decrease in impurity\")\n",
    "\n",
    "# Logarithmische Skala für die x-Achse\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Werte neben den Balken anzeigen\n",
    "for i, (feature, importance, uncertainty) in enumerate(zip(forest_importances_Temp.index, forest_importances_Temp.values, std_Temp)):\n",
    "    ax.text(importance + uncertainty + 0.1, i, f'{importance:.2f}% ±{uncertainty:.2f}%', ha='left', va='center')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Breite des Bildes anpassen\n",
    "fig, ax = plt.subplots(figsize=(8, 30))\n",
    "\n",
    "# Balkenplot mit Fehlerbalken\n",
    "forest_importances_Reg.plot.barh(xerr=std_Reg, ax=ax, capsize=4)\n",
    "\n",
    "# Titel und Achsenbeschriftungen\n",
    "ax.set_title(\"Feature importances with uncertainty\")\n",
    "ax.set_xlabel(\"Mean decrease in impurity\")\n",
    "\n",
    "# Logarithmische Skala für die x-Achse\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Werte neben den Balken anzeigen\n",
    "for i, (feature, importance, uncertainty) in enumerate(zip(forest_importances_Reg.index, forest_importances_Reg.values, std_Reg)):\n",
    "    ax.text(importance + uncertainty + 0.1, i, f'{importance:.2f}% ±{uncertainty:.2f}%', ha='left', va='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4793e4-cbd4-462a-b179-851119e6a4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e6f219-e467-4f28-8c93-80342aac4d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
